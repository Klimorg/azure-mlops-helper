{"config":{"indexing":"full","lang":["fr"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Azure MLOPs Helper Documentation. Introduction This library is intended to ease the various steps needed to enable the storage, training, and deployment of a model with Azure Machine Learning. This is highly inspired from the series of posts found in the following blog . This is mostly a wrapper around azure-ml python classes and some glue code. Remark If you know what you are doing with AzureML, you'll probably don't need this library. Resources Open source cheat sheets for Azure ML","title":"Accueil"},{"location":"#welcome-to-azure-mlops-helper-documentation","text":"","title":"Welcome to Azure MLOPs Helper Documentation."},{"location":"#introduction","text":"This library is intended to ease the various steps needed to enable the storage, training, and deployment of a model with Azure Machine Learning. This is highly inspired from the series of posts found in the following blog . This is mostly a wrapper around azure-ml python classes and some glue code. Remark If you know what you are doing with AzureML, you'll probably don't need this library.","title":"Introduction"},{"location":"#resources","text":"Open source cheat sheets for Azure ML","title":"Resources"},{"location":"steps/create_aml_env/","text":"Azure Machine Learning Environment Creation azure_helper.steps.create_aml_env EnvSpecs Bases: BaseModel Pydantic class used to specify how to build the environment : with pip, conda or docker. Parameters: Name Type Description Default flavor str Which method you chose, either pip, conda of docker. required spec_file Path The path to either the requirements.txt files, the environment.yml file, or the Dockerfile . required Source code in azure_helper/steps/create_aml_env.py 12 13 14 15 16 17 18 19 20 21 class EnvSpecs ( BaseModel ): \"\"\"Pydantic class used to specify how to build the environment : with pip, conda or docker. Args: flavor (str): Which method you chose, either pip, conda of docker. spec_file (Path): The path to either the `requirements.txt` files, the `environment.yml` file, or the `Dockerfile`. \"\"\" flavor : str spec_file : Path AMLEnvironment Source code in azure_helper/steps/create_aml_env.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 class AMLEnvironment : def __init__ ( self , private_wheel_location : Path , base_image : str ) -> None : \"\"\"Instantiate the creation of the AzureML Environment needed for the experiment. An AzureML Environment is a Docker Image which encapsulates all the needed requirements for the experiment (ie the training of a model) to run, this can be : * the version of the OS, * the various python libraries needed (ie the `requirements.txt`), * the project built as a wheel and added as a private pip package, * third-party softwares, etc. To create the Docker image of the environment, we use a base image managed by Microsoft. This image can be changed corresponding to the need of the experiment. The standard image we use is `mcr.microsoft.com/azureml/curated/sklearn-1.0-ubuntu20.04-py38-cpu`. The different images managed by Microsoft can be found ine the [Azure Machine Learning base images](https://github.com/Azure/AzureML-Containers). The create the Docker image of the environment, we also need the project, built as a pip wheel (by Flit, setuptools, Hatch, etc), so we can install it in our environment. Args: dist_dir (Path): The path where the project built as a wheel is located. base_image (str): The base Docker image we use to create the environment. \"\"\" self . private_wheel_location = private_wheel_location self . base_image = base_image def validate_dir ( self ): \"\"\"Small validation to check if a given path is a valid path in the project. Raises: FileNotFoundError: The path does not point to a valid directory. \"\"\" if self . private_wheel_location . is_dir (): log . info ( f \"Looking for wheel file in { self . private_wheel_location } .\" ) else : raise FileNotFoundError def retrieve_whl_filepath ( self ) -> Path : \"\"\"The project inside you develop your model you want to train need to be build as a pip wheel and added to the training environement. Once your project has been build as a pip wheel (by Flit, setuptools, Hatch, etc), given the location of the distribution directory (usually `$cwd/dist`), this function looks for a `.whl` file and return its path. !!! remark \"Remark\" Usually, there is only one `.whl` file in a `dist` directory. This is an assumption of this function. Raises: FileNotFoundError: Either the `dist` directory path is not valid, or there is no `.whl` file inside. Returns: Path: The path of the private wheel package. \"\"\" try : self . validate_dir () except FileNotFoundError : log . error ( f \"Couldn't find distribution directory { self . private_wheel_location } \" , ) raise whl_file = sorted ( Path ( file ) for file in Path ( self . private_wheel_location ) . glob ( \"**/*.whl\" ) if file . is_file () ) if len ( whl_file ) == 0 : log . error ( \"Couldn't find wheel distribution\" ) raise FileNotFoundError log . info ( f \"Found wheel { self . private_wheel_location / Path ( whl_file [ 0 ]) } \" ) return self . private_wheel_location / Path ( whl_file [ 0 ]) def create_aml_environment ( self , env_name : str , env_specs : EnvSpecs , aml_interface : AMLInterface , ) -> Environment : \"\"\"Create the AzureML Environment once all the requirements have been gathered. Args: env_name (str): The name of the environment that will be build. env_specs (EnvSpecs): The specifications used to create the environement (ie pip, conda, or docker). aml_interface (AMLInterface): The AML interface which will be responsible to register the environment in the right workspace. Raises: ValueError: You have selected something else than \"pip\", \"conda\", or \"docker\" for the `EnvSpecs`. Returns: Environment: The training environment which will be used. !!! remark \"Remark\" The environment returned is just a Dockerfile, it not built as an image and will have to be built during the first training. If you want to build it locally, you will have to put `build_locally=True` in `AMLInterface.register_aml_environment`. \"\"\" if env_specs . flavor == \"pip\" : env = Environment . from_pip_requirements ( name = env_name , file_path = str ( env_specs . spec_file ), ) elif env_specs . flavor == \"conda\" : env = Environment . from_conda_specification ( name = env_name , file_path = str ( env_specs . spec_file ), ) elif env_specs . flavor == \"docker\" : env = Environment . from_dockerfile ( name = env_name , dockerfile = str ( env_specs . spec_file ), ) else : log . error ( f \"env_specs flavor { env_specs . flavor } is not a valid one. Only 'pip', 'conda', or 'docker' are valid choices.\" , ) raise ValueError whl_filepath = self . retrieve_whl_filepath () private_wheel = env . add_private_pip_wheel ( workspace = aml_interface . workspace , file_path = whl_filepath , exist_ok = True , ) env . python . conda_dependencies . add_pip_package ( private_wheel ) env . docker . base_image = self . base_image # env.python.user_managed_dependencies = True # https://stackoverflow.com/questions/67387249/how-to-use-azureml-core-runconfig-dockerconfiguration-class-in-azureml-core-envi return env __init__ ( private_wheel_location , base_image ) Instantiate the creation of the AzureML Environment needed for the experiment. An AzureML Environment is a Docker Image which encapsulates all the needed requirements for the experiment (ie the training of a model) to run, this can be : the version of the OS, the various python libraries needed (ie the requirements.txt ), the project built as a wheel and added as a private pip package, third-party softwares, etc. To create the Docker image of the environment, we use a base image managed by Microsoft. This image can be changed corresponding to the need of the experiment. The standard image we use is mcr.microsoft.com/azureml/curated/sklearn-1.0-ubuntu20.04-py38-cpu . The different images managed by Microsoft can be found ine the Azure Machine Learning base images . The create the Docker image of the environment, we also need the project, built as a pip wheel (by Flit, setuptools, Hatch, etc), so we can install it in our environment. Parameters: Name Type Description Default dist_dir Path The path where the project built as a wheel is located. required base_image str The base Docker image we use to create the environment. required Source code in azure_helper/steps/create_aml_env.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def __init__ ( self , private_wheel_location : Path , base_image : str ) -> None : \"\"\"Instantiate the creation of the AzureML Environment needed for the experiment. An AzureML Environment is a Docker Image which encapsulates all the needed requirements for the experiment (ie the training of a model) to run, this can be : * the version of the OS, * the various python libraries needed (ie the `requirements.txt`), * the project built as a wheel and added as a private pip package, * third-party softwares, etc. To create the Docker image of the environment, we use a base image managed by Microsoft. This image can be changed corresponding to the need of the experiment. The standard image we use is `mcr.microsoft.com/azureml/curated/sklearn-1.0-ubuntu20.04-py38-cpu`. The different images managed by Microsoft can be found ine the [Azure Machine Learning base images](https://github.com/Azure/AzureML-Containers). The create the Docker image of the environment, we also need the project, built as a pip wheel (by Flit, setuptools, Hatch, etc), so we can install it in our environment. Args: dist_dir (Path): The path where the project built as a wheel is located. base_image (str): The base Docker image we use to create the environment. \"\"\" self . private_wheel_location = private_wheel_location self . base_image = base_image validate_dir () Small validation to check if a given path is a valid path in the project. Raises: Type Description FileNotFoundError The path does not point to a valid directory. Source code in azure_helper/steps/create_aml_env.py 53 54 55 56 57 58 59 60 61 62 def validate_dir ( self ): \"\"\"Small validation to check if a given path is a valid path in the project. Raises: FileNotFoundError: The path does not point to a valid directory. \"\"\" if self . private_wheel_location . is_dir (): log . info ( f \"Looking for wheel file in { self . private_wheel_location } .\" ) else : raise FileNotFoundError retrieve_whl_filepath () The project inside you develop your model you want to train need to be build as a pip wheel and added to the training environement. Once your project has been build as a pip wheel (by Flit, setuptools, Hatch, etc), given the location of the distribution directory (usually $cwd/dist ), this function looks for a .whl file and return its path. Remark Usually, there is only one .whl file in a dist directory. This is an assumption of this function. Raises: Type Description FileNotFoundError Either the dist directory path is not valid, or there is no .whl file inside. Returns: Name Type Description Path Path The path of the private wheel package. Source code in azure_helper/steps/create_aml_env.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def retrieve_whl_filepath ( self ) -> Path : \"\"\"The project inside you develop your model you want to train need to be build as a pip wheel and added to the training environement. Once your project has been build as a pip wheel (by Flit, setuptools, Hatch, etc), given the location of the distribution directory (usually `$cwd/dist`), this function looks for a `.whl` file and return its path. !!! remark \"Remark\" Usually, there is only one `.whl` file in a `dist` directory. This is an assumption of this function. Raises: FileNotFoundError: Either the `dist` directory path is not valid, or there is no `.whl` file inside. Returns: Path: The path of the private wheel package. \"\"\" try : self . validate_dir () except FileNotFoundError : log . error ( f \"Couldn't find distribution directory { self . private_wheel_location } \" , ) raise whl_file = sorted ( Path ( file ) for file in Path ( self . private_wheel_location ) . glob ( \"**/*.whl\" ) if file . is_file () ) if len ( whl_file ) == 0 : log . error ( \"Couldn't find wheel distribution\" ) raise FileNotFoundError log . info ( f \"Found wheel { self . private_wheel_location / Path ( whl_file [ 0 ]) } \" ) return self . private_wheel_location / Path ( whl_file [ 0 ]) create_aml_environment ( env_name , env_specs , aml_interface ) Create the AzureML Environment once all the requirements have been gathered. Parameters: Name Type Description Default env_name str The name of the environment that will be build. required env_specs EnvSpecs The specifications used to create the environement (ie pip, conda, or docker). required aml_interface AMLInterface The AML interface which will be responsible to register the environment in the right workspace. required Raises: Type Description ValueError You have selected something else than \"pip\", \"conda\", or \"docker\" for the EnvSpecs . Returns: Name Type Description Environment Environment The training environment which will be used. Remark The environment returned is just a Dockerfile, it not built as an image and will have to be built during the first training. If you want to build it locally, you will have to put build_locally=True in AMLInterface.register_aml_environment . Source code in azure_helper/steps/create_aml_env.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 def create_aml_environment ( self , env_name : str , env_specs : EnvSpecs , aml_interface : AMLInterface , ) -> Environment : \"\"\"Create the AzureML Environment once all the requirements have been gathered. Args: env_name (str): The name of the environment that will be build. env_specs (EnvSpecs): The specifications used to create the environement (ie pip, conda, or docker). aml_interface (AMLInterface): The AML interface which will be responsible to register the environment in the right workspace. Raises: ValueError: You have selected something else than \"pip\", \"conda\", or \"docker\" for the `EnvSpecs`. Returns: Environment: The training environment which will be used. !!! remark \"Remark\" The environment returned is just a Dockerfile, it not built as an image and will have to be built during the first training. If you want to build it locally, you will have to put `build_locally=True` in `AMLInterface.register_aml_environment`. \"\"\" if env_specs . flavor == \"pip\" : env = Environment . from_pip_requirements ( name = env_name , file_path = str ( env_specs . spec_file ), ) elif env_specs . flavor == \"conda\" : env = Environment . from_conda_specification ( name = env_name , file_path = str ( env_specs . spec_file ), ) elif env_specs . flavor == \"docker\" : env = Environment . from_dockerfile ( name = env_name , dockerfile = str ( env_specs . spec_file ), ) else : log . error ( f \"env_specs flavor { env_specs . flavor } is not a valid one. Only 'pip', 'conda', or 'docker' are valid choices.\" , ) raise ValueError whl_filepath = self . retrieve_whl_filepath () private_wheel = env . add_private_pip_wheel ( workspace = aml_interface . workspace , file_path = whl_filepath , exist_ok = True , ) env . python . conda_dependencies . add_pip_package ( private_wheel ) env . docker . base_image = self . base_image # env.python.user_managed_dependencies = True # https://stackoverflow.com/questions/67387249/how-to-use-azureml-core-runconfig-dockerconfiguration-class-in-azureml-core-envi return env","title":"Environment Creation"},{"location":"steps/create_aml_env/#azure-machine-learning-environment-creation","text":"","title":"Azure Machine Learning Environment Creation"},{"location":"steps/create_aml_env/#azure_helper.steps.create_aml_env","text":"","title":"create_aml_env"},{"location":"steps/create_aml_env/#azure_helper.steps.create_aml_env.EnvSpecs","text":"Bases: BaseModel Pydantic class used to specify how to build the environment : with pip, conda or docker. Parameters: Name Type Description Default flavor str Which method you chose, either pip, conda of docker. required spec_file Path The path to either the requirements.txt files, the environment.yml file, or the Dockerfile . required Source code in azure_helper/steps/create_aml_env.py 12 13 14 15 16 17 18 19 20 21 class EnvSpecs ( BaseModel ): \"\"\"Pydantic class used to specify how to build the environment : with pip, conda or docker. Args: flavor (str): Which method you chose, either pip, conda of docker. spec_file (Path): The path to either the `requirements.txt` files, the `environment.yml` file, or the `Dockerfile`. \"\"\" flavor : str spec_file : Path","title":"EnvSpecs"},{"location":"steps/create_aml_env/#azure_helper.steps.create_aml_env.AMLEnvironment","text":"Source code in azure_helper/steps/create_aml_env.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 class AMLEnvironment : def __init__ ( self , private_wheel_location : Path , base_image : str ) -> None : \"\"\"Instantiate the creation of the AzureML Environment needed for the experiment. An AzureML Environment is a Docker Image which encapsulates all the needed requirements for the experiment (ie the training of a model) to run, this can be : * the version of the OS, * the various python libraries needed (ie the `requirements.txt`), * the project built as a wheel and added as a private pip package, * third-party softwares, etc. To create the Docker image of the environment, we use a base image managed by Microsoft. This image can be changed corresponding to the need of the experiment. The standard image we use is `mcr.microsoft.com/azureml/curated/sklearn-1.0-ubuntu20.04-py38-cpu`. The different images managed by Microsoft can be found ine the [Azure Machine Learning base images](https://github.com/Azure/AzureML-Containers). The create the Docker image of the environment, we also need the project, built as a pip wheel (by Flit, setuptools, Hatch, etc), so we can install it in our environment. Args: dist_dir (Path): The path where the project built as a wheel is located. base_image (str): The base Docker image we use to create the environment. \"\"\" self . private_wheel_location = private_wheel_location self . base_image = base_image def validate_dir ( self ): \"\"\"Small validation to check if a given path is a valid path in the project. Raises: FileNotFoundError: The path does not point to a valid directory. \"\"\" if self . private_wheel_location . is_dir (): log . info ( f \"Looking for wheel file in { self . private_wheel_location } .\" ) else : raise FileNotFoundError def retrieve_whl_filepath ( self ) -> Path : \"\"\"The project inside you develop your model you want to train need to be build as a pip wheel and added to the training environement. Once your project has been build as a pip wheel (by Flit, setuptools, Hatch, etc), given the location of the distribution directory (usually `$cwd/dist`), this function looks for a `.whl` file and return its path. !!! remark \"Remark\" Usually, there is only one `.whl` file in a `dist` directory. This is an assumption of this function. Raises: FileNotFoundError: Either the `dist` directory path is not valid, or there is no `.whl` file inside. Returns: Path: The path of the private wheel package. \"\"\" try : self . validate_dir () except FileNotFoundError : log . error ( f \"Couldn't find distribution directory { self . private_wheel_location } \" , ) raise whl_file = sorted ( Path ( file ) for file in Path ( self . private_wheel_location ) . glob ( \"**/*.whl\" ) if file . is_file () ) if len ( whl_file ) == 0 : log . error ( \"Couldn't find wheel distribution\" ) raise FileNotFoundError log . info ( f \"Found wheel { self . private_wheel_location / Path ( whl_file [ 0 ]) } \" ) return self . private_wheel_location / Path ( whl_file [ 0 ]) def create_aml_environment ( self , env_name : str , env_specs : EnvSpecs , aml_interface : AMLInterface , ) -> Environment : \"\"\"Create the AzureML Environment once all the requirements have been gathered. Args: env_name (str): The name of the environment that will be build. env_specs (EnvSpecs): The specifications used to create the environement (ie pip, conda, or docker). aml_interface (AMLInterface): The AML interface which will be responsible to register the environment in the right workspace. Raises: ValueError: You have selected something else than \"pip\", \"conda\", or \"docker\" for the `EnvSpecs`. Returns: Environment: The training environment which will be used. !!! remark \"Remark\" The environment returned is just a Dockerfile, it not built as an image and will have to be built during the first training. If you want to build it locally, you will have to put `build_locally=True` in `AMLInterface.register_aml_environment`. \"\"\" if env_specs . flavor == \"pip\" : env = Environment . from_pip_requirements ( name = env_name , file_path = str ( env_specs . spec_file ), ) elif env_specs . flavor == \"conda\" : env = Environment . from_conda_specification ( name = env_name , file_path = str ( env_specs . spec_file ), ) elif env_specs . flavor == \"docker\" : env = Environment . from_dockerfile ( name = env_name , dockerfile = str ( env_specs . spec_file ), ) else : log . error ( f \"env_specs flavor { env_specs . flavor } is not a valid one. Only 'pip', 'conda', or 'docker' are valid choices.\" , ) raise ValueError whl_filepath = self . retrieve_whl_filepath () private_wheel = env . add_private_pip_wheel ( workspace = aml_interface . workspace , file_path = whl_filepath , exist_ok = True , ) env . python . conda_dependencies . add_pip_package ( private_wheel ) env . docker . base_image = self . base_image # env.python.user_managed_dependencies = True # https://stackoverflow.com/questions/67387249/how-to-use-azureml-core-runconfig-dockerconfiguration-class-in-azureml-core-envi return env","title":"AMLEnvironment"},{"location":"steps/create_aml_env/#azure_helper.steps.create_aml_env.AMLEnvironment.__init__","text":"Instantiate the creation of the AzureML Environment needed for the experiment. An AzureML Environment is a Docker Image which encapsulates all the needed requirements for the experiment (ie the training of a model) to run, this can be : the version of the OS, the various python libraries needed (ie the requirements.txt ), the project built as a wheel and added as a private pip package, third-party softwares, etc. To create the Docker image of the environment, we use a base image managed by Microsoft. This image can be changed corresponding to the need of the experiment. The standard image we use is mcr.microsoft.com/azureml/curated/sklearn-1.0-ubuntu20.04-py38-cpu . The different images managed by Microsoft can be found ine the Azure Machine Learning base images . The create the Docker image of the environment, we also need the project, built as a pip wheel (by Flit, setuptools, Hatch, etc), so we can install it in our environment. Parameters: Name Type Description Default dist_dir Path The path where the project built as a wheel is located. required base_image str The base Docker image we use to create the environment. required Source code in azure_helper/steps/create_aml_env.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def __init__ ( self , private_wheel_location : Path , base_image : str ) -> None : \"\"\"Instantiate the creation of the AzureML Environment needed for the experiment. An AzureML Environment is a Docker Image which encapsulates all the needed requirements for the experiment (ie the training of a model) to run, this can be : * the version of the OS, * the various python libraries needed (ie the `requirements.txt`), * the project built as a wheel and added as a private pip package, * third-party softwares, etc. To create the Docker image of the environment, we use a base image managed by Microsoft. This image can be changed corresponding to the need of the experiment. The standard image we use is `mcr.microsoft.com/azureml/curated/sklearn-1.0-ubuntu20.04-py38-cpu`. The different images managed by Microsoft can be found ine the [Azure Machine Learning base images](https://github.com/Azure/AzureML-Containers). The create the Docker image of the environment, we also need the project, built as a pip wheel (by Flit, setuptools, Hatch, etc), so we can install it in our environment. Args: dist_dir (Path): The path where the project built as a wheel is located. base_image (str): The base Docker image we use to create the environment. \"\"\" self . private_wheel_location = private_wheel_location self . base_image = base_image","title":"__init__()"},{"location":"steps/create_aml_env/#azure_helper.steps.create_aml_env.AMLEnvironment.validate_dir","text":"Small validation to check if a given path is a valid path in the project. Raises: Type Description FileNotFoundError The path does not point to a valid directory. Source code in azure_helper/steps/create_aml_env.py 53 54 55 56 57 58 59 60 61 62 def validate_dir ( self ): \"\"\"Small validation to check if a given path is a valid path in the project. Raises: FileNotFoundError: The path does not point to a valid directory. \"\"\" if self . private_wheel_location . is_dir (): log . info ( f \"Looking for wheel file in { self . private_wheel_location } .\" ) else : raise FileNotFoundError","title":"validate_dir()"},{"location":"steps/create_aml_env/#azure_helper.steps.create_aml_env.AMLEnvironment.retrieve_whl_filepath","text":"The project inside you develop your model you want to train need to be build as a pip wheel and added to the training environement. Once your project has been build as a pip wheel (by Flit, setuptools, Hatch, etc), given the location of the distribution directory (usually $cwd/dist ), this function looks for a .whl file and return its path. Remark Usually, there is only one .whl file in a dist directory. This is an assumption of this function. Raises: Type Description FileNotFoundError Either the dist directory path is not valid, or there is no .whl file inside. Returns: Name Type Description Path Path The path of the private wheel package. Source code in azure_helper/steps/create_aml_env.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def retrieve_whl_filepath ( self ) -> Path : \"\"\"The project inside you develop your model you want to train need to be build as a pip wheel and added to the training environement. Once your project has been build as a pip wheel (by Flit, setuptools, Hatch, etc), given the location of the distribution directory (usually `$cwd/dist`), this function looks for a `.whl` file and return its path. !!! remark \"Remark\" Usually, there is only one `.whl` file in a `dist` directory. This is an assumption of this function. Raises: FileNotFoundError: Either the `dist` directory path is not valid, or there is no `.whl` file inside. Returns: Path: The path of the private wheel package. \"\"\" try : self . validate_dir () except FileNotFoundError : log . error ( f \"Couldn't find distribution directory { self . private_wheel_location } \" , ) raise whl_file = sorted ( Path ( file ) for file in Path ( self . private_wheel_location ) . glob ( \"**/*.whl\" ) if file . is_file () ) if len ( whl_file ) == 0 : log . error ( \"Couldn't find wheel distribution\" ) raise FileNotFoundError log . info ( f \"Found wheel { self . private_wheel_location / Path ( whl_file [ 0 ]) } \" ) return self . private_wheel_location / Path ( whl_file [ 0 ])","title":"retrieve_whl_filepath()"},{"location":"steps/create_aml_env/#azure_helper.steps.create_aml_env.AMLEnvironment.create_aml_environment","text":"Create the AzureML Environment once all the requirements have been gathered. Parameters: Name Type Description Default env_name str The name of the environment that will be build. required env_specs EnvSpecs The specifications used to create the environement (ie pip, conda, or docker). required aml_interface AMLInterface The AML interface which will be responsible to register the environment in the right workspace. required Raises: Type Description ValueError You have selected something else than \"pip\", \"conda\", or \"docker\" for the EnvSpecs . Returns: Name Type Description Environment Environment The training environment which will be used. Remark The environment returned is just a Dockerfile, it not built as an image and will have to be built during the first training. If you want to build it locally, you will have to put build_locally=True in AMLInterface.register_aml_environment . Source code in azure_helper/steps/create_aml_env.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 def create_aml_environment ( self , env_name : str , env_specs : EnvSpecs , aml_interface : AMLInterface , ) -> Environment : \"\"\"Create the AzureML Environment once all the requirements have been gathered. Args: env_name (str): The name of the environment that will be build. env_specs (EnvSpecs): The specifications used to create the environement (ie pip, conda, or docker). aml_interface (AMLInterface): The AML interface which will be responsible to register the environment in the right workspace. Raises: ValueError: You have selected something else than \"pip\", \"conda\", or \"docker\" for the `EnvSpecs`. Returns: Environment: The training environment which will be used. !!! remark \"Remark\" The environment returned is just a Dockerfile, it not built as an image and will have to be built during the first training. If you want to build it locally, you will have to put `build_locally=True` in `AMLInterface.register_aml_environment`. \"\"\" if env_specs . flavor == \"pip\" : env = Environment . from_pip_requirements ( name = env_name , file_path = str ( env_specs . spec_file ), ) elif env_specs . flavor == \"conda\" : env = Environment . from_conda_specification ( name = env_name , file_path = str ( env_specs . spec_file ), ) elif env_specs . flavor == \"docker\" : env = Environment . from_dockerfile ( name = env_name , dockerfile = str ( env_specs . spec_file ), ) else : log . error ( f \"env_specs flavor { env_specs . flavor } is not a valid one. Only 'pip', 'conda', or 'docker' are valid choices.\" , ) raise ValueError whl_filepath = self . retrieve_whl_filepath () private_wheel = env . add_private_pip_wheel ( workspace = aml_interface . workspace , file_path = whl_filepath , exist_ok = True , ) env . python . conda_dependencies . add_pip_package ( private_wheel ) env . docker . base_image = self . base_image # env.python.user_managed_dependencies = True # https://stackoverflow.com/questions/67387249/how-to-use-azureml-core-runconfig-dockerconfiguration-class-in-azureml-core-envi return env","title":"create_aml_environment()"},{"location":"steps/create_aml_experiment/","text":"Azure Machine Learning Experiment Creation AMLExperiment Source code in azure_helper/steps/create_aml_experiment.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 class AMLExperiment : def __init__ ( self , aml_interface : AMLInterface , aml_compute_name : str , aml_compute_instance : str , env_name : str , experiment_name : str , training_script_path : str , min_node : int = 1 , max_node : int = 2 , clean_after_run : bool = True , ) -> None : \"\"\"Instantiate the creation of an AzureML Experiment needed to train a model. Args: aml_interface (AMLInterface): The aml_interface needed to register the experiment in the workspace. aml_compute_name (str): The name of the compute instance used. aml_compute_instance (str): The size (as in `vm_size`) of the compute instance used. min_node (int, optional): The minimum number of nodes to use on the compute instance. Defaults to 1. max_node (int, optional): The maximum number of nodes to use on the compute instance. Defaults to 2. env_name (str): The name of the training environment. experiment_name (str): The name of the experiment. training_script_path (str): The path to the training loop script. clean_after_run (bool, optional): Whether or not you want to delete the compute instance after training. Defaults to True. \"\"\" self . interface = aml_interface self . aml_compute_name = aml_compute_name self . aml_compute_instance = aml_compute_instance self . env_name = env_name self . experiment_name = experiment_name self . clean_after_run = clean_after_run self . training_script_path = training_script_path self . compute_target = aml_interface . get_compute_target ( aml_compute_name , aml_compute_instance , min_node , max_node , ) def generate_run_config ( self ) -> RunConfiguration : \"\"\"Generate the run configuration of the experiment. By definition, the run configuration is the combination of the training environment and the compute instance. Returns: RunConfiguration: The run configuration of the experiment. \"\"\" run_config = RunConfiguration () docker_config = DockerConfiguration ( use_docker = True ) run_config . docker = docker_config aml_run_env = Environment . get ( self . interface . workspace , self . env_name , ) run_config . environment = aml_run_env run_config . target = self . compute_target return run_config def submit_pipeline ( self , steps : List [ PythonScriptStep ]): \"\"\"Used to submit a training pipeline. Args: steps (List[PythonScriptStep]): The different steps of the training pipeline. https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb This should be a registered AZML Pipeline with the followings steps : * Download raw datas from one datastore * Transform those datas in \"gold\" datas and store them in another datastore * Use these gold datas to train a model * Evaluate that model * Save and version that model \"\"\" experiment = Experiment ( self . interface . workspace , self . experiment_name ) # src_dir = __here__ # src_dir = str(Path.cwd()) pipeline = Pipeline ( workspace = self . interface . workspace , steps = steps ) # TODO: add the validation method of the pipeline. log . info ( \"Submitting Run\" ) run = experiment . submit ( config = pipeline ) run . wait_for_completion ( show_output = True ) log . info ( \"Run completed.\" ) if self . clean_after_run : log . info ( \"Deleting compute instance.\" ) self . compute_target . delete () def submit_run ( self ): \"\"\"Submit your training loop and create an experiment. This experiment is defined by the use of the `ScriptRunConfig` class. This means that you have to provide the path to a script defining your training loop, defined by the parameters `source_directory` and `script` of the `ScriptRunConfig` class, `script` being the path of your training loop relative to `source_directory`. For example purpose, a training loop example is provided [TrainingLoopExample][azure_helper.steps.train] is provided, as well as an abstract class if you want to use this training loop structure, but you're not forced to. \"\"\" experiment = Experiment ( self . interface . workspace , self . experiment_name ) # src_dir = __here__ src_dir = str ( Path . cwd ()) docker_config = DockerConfiguration ( use_docker = True ) script = ScriptRunConfig ( source_directory = src_dir , script = self . training_script_path , docker_runtime_config = docker_config , ) script . run_config = self . generate_run_config () # script.run_config.target = self.compute_target # aml_run_env = Environment.get( # self.interface.workspace, # self.env_name, # ) # script.run_config.environment = aml_run_env log . info ( \"Submitting Run\" ) run = experiment . submit ( config = script ) run . wait_for_completion ( show_output = True ) log . info ( f \"Run completed : { run . get_metrics () } \" ) if self . clean_after_run : log . info ( \"Deleting compute instance.\" ) self . compute_target . delete () __init__ ( aml_interface , aml_compute_name , aml_compute_instance , env_name , experiment_name , training_script_path , min_node = 1 , max_node = 2 , clean_after_run = True ) Instantiate the creation of an AzureML Experiment needed to train a model. Parameters: Name Type Description Default aml_interface AMLInterface The aml_interface needed to register the experiment in the workspace. required aml_compute_name str The name of the compute instance used. required aml_compute_instance str The size (as in vm_size ) of the compute instance used. required min_node int The minimum number of nodes to use on the compute instance. Defaults to 1. 1 max_node int The maximum number of nodes to use on the compute instance. Defaults to 2. 2 env_name str The name of the training environment. required experiment_name str The name of the experiment. required training_script_path str The path to the training loop script. required clean_after_run bool Whether or not you want to delete the compute instance after training. Defaults to True. True Source code in azure_helper/steps/create_aml_experiment.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def __init__ ( self , aml_interface : AMLInterface , aml_compute_name : str , aml_compute_instance : str , env_name : str , experiment_name : str , training_script_path : str , min_node : int = 1 , max_node : int = 2 , clean_after_run : bool = True , ) -> None : \"\"\"Instantiate the creation of an AzureML Experiment needed to train a model. Args: aml_interface (AMLInterface): The aml_interface needed to register the experiment in the workspace. aml_compute_name (str): The name of the compute instance used. aml_compute_instance (str): The size (as in `vm_size`) of the compute instance used. min_node (int, optional): The minimum number of nodes to use on the compute instance. Defaults to 1. max_node (int, optional): The maximum number of nodes to use on the compute instance. Defaults to 2. env_name (str): The name of the training environment. experiment_name (str): The name of the experiment. training_script_path (str): The path to the training loop script. clean_after_run (bool, optional): Whether or not you want to delete the compute instance after training. Defaults to True. \"\"\" self . interface = aml_interface self . aml_compute_name = aml_compute_name self . aml_compute_instance = aml_compute_instance self . env_name = env_name self . experiment_name = experiment_name self . clean_after_run = clean_after_run self . training_script_path = training_script_path self . compute_target = aml_interface . get_compute_target ( aml_compute_name , aml_compute_instance , min_node , max_node , ) generate_run_config () Generate the run configuration of the experiment. By definition, the run configuration is the combination of the training environment and the compute instance. Returns: Name Type Description RunConfiguration RunConfiguration The run configuration of the experiment. Source code in azure_helper/steps/create_aml_experiment.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def generate_run_config ( self ) -> RunConfiguration : \"\"\"Generate the run configuration of the experiment. By definition, the run configuration is the combination of the training environment and the compute instance. Returns: RunConfiguration: The run configuration of the experiment. \"\"\" run_config = RunConfiguration () docker_config = DockerConfiguration ( use_docker = True ) run_config . docker = docker_config aml_run_env = Environment . get ( self . interface . workspace , self . env_name , ) run_config . environment = aml_run_env run_config . target = self . compute_target return run_config submit_pipeline ( steps ) Used to submit a training pipeline. Parameters: Name Type Description Default steps List [ PythonScriptStep ] The different steps of the training pipeline. required This should be a registered AZML Pipeline with the followings steps : Download raw datas from one datastore Transform those datas in \"gold\" datas and store them in another datastore Use these gold datas to train a model Evaluate that model Save and version that model Source code in azure_helper/steps/create_aml_experiment.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def submit_pipeline ( self , steps : List [ PythonScriptStep ]): \"\"\"Used to submit a training pipeline. Args: steps (List[PythonScriptStep]): The different steps of the training pipeline. https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb This should be a registered AZML Pipeline with the followings steps : * Download raw datas from one datastore * Transform those datas in \"gold\" datas and store them in another datastore * Use these gold datas to train a model * Evaluate that model * Save and version that model \"\"\" experiment = Experiment ( self . interface . workspace , self . experiment_name ) # src_dir = __here__ # src_dir = str(Path.cwd()) pipeline = Pipeline ( workspace = self . interface . workspace , steps = steps ) # TODO: add the validation method of the pipeline. log . info ( \"Submitting Run\" ) run = experiment . submit ( config = pipeline ) run . wait_for_completion ( show_output = True ) log . info ( \"Run completed.\" ) if self . clean_after_run : log . info ( \"Deleting compute instance.\" ) self . compute_target . delete () submit_run () Submit your training loop and create an experiment. This experiment is defined by the use of the ScriptRunConfig class. This means that you have to provide the path to a script defining your training loop, defined by the parameters source_directory and script of the ScriptRunConfig class, script being the path of your training loop relative to source_directory . For example purpose, a training loop example is provided TrainingLoopExample is provided, as well as an abstract class if you want to use this training loop structure, but you're not forced to. Source code in azure_helper/steps/create_aml_experiment.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def submit_run ( self ): \"\"\"Submit your training loop and create an experiment. This experiment is defined by the use of the `ScriptRunConfig` class. This means that you have to provide the path to a script defining your training loop, defined by the parameters `source_directory` and `script` of the `ScriptRunConfig` class, `script` being the path of your training loop relative to `source_directory`. For example purpose, a training loop example is provided [TrainingLoopExample][azure_helper.steps.train] is provided, as well as an abstract class if you want to use this training loop structure, but you're not forced to. \"\"\" experiment = Experiment ( self . interface . workspace , self . experiment_name ) # src_dir = __here__ src_dir = str ( Path . cwd ()) docker_config = DockerConfiguration ( use_docker = True ) script = ScriptRunConfig ( source_directory = src_dir , script = self . training_script_path , docker_runtime_config = docker_config , ) script . run_config = self . generate_run_config () # script.run_config.target = self.compute_target # aml_run_env = Environment.get( # self.interface.workspace, # self.env_name, # ) # script.run_config.environment = aml_run_env log . info ( \"Submitting Run\" ) run = experiment . submit ( config = script ) run . wait_for_completion ( show_output = True ) log . info ( f \"Run completed : { run . get_metrics () } \" ) if self . clean_after_run : log . info ( \"Deleting compute instance.\" ) self . compute_target . delete ()","title":"Create Experiment"},{"location":"steps/create_aml_experiment/#azure-machine-learning-experiment-creation","text":"","title":"Azure Machine Learning Experiment Creation"},{"location":"steps/create_aml_experiment/#azure_helper.steps.create_aml_experiment.AMLExperiment","text":"Source code in azure_helper/steps/create_aml_experiment.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 class AMLExperiment : def __init__ ( self , aml_interface : AMLInterface , aml_compute_name : str , aml_compute_instance : str , env_name : str , experiment_name : str , training_script_path : str , min_node : int = 1 , max_node : int = 2 , clean_after_run : bool = True , ) -> None : \"\"\"Instantiate the creation of an AzureML Experiment needed to train a model. Args: aml_interface (AMLInterface): The aml_interface needed to register the experiment in the workspace. aml_compute_name (str): The name of the compute instance used. aml_compute_instance (str): The size (as in `vm_size`) of the compute instance used. min_node (int, optional): The minimum number of nodes to use on the compute instance. Defaults to 1. max_node (int, optional): The maximum number of nodes to use on the compute instance. Defaults to 2. env_name (str): The name of the training environment. experiment_name (str): The name of the experiment. training_script_path (str): The path to the training loop script. clean_after_run (bool, optional): Whether or not you want to delete the compute instance after training. Defaults to True. \"\"\" self . interface = aml_interface self . aml_compute_name = aml_compute_name self . aml_compute_instance = aml_compute_instance self . env_name = env_name self . experiment_name = experiment_name self . clean_after_run = clean_after_run self . training_script_path = training_script_path self . compute_target = aml_interface . get_compute_target ( aml_compute_name , aml_compute_instance , min_node , max_node , ) def generate_run_config ( self ) -> RunConfiguration : \"\"\"Generate the run configuration of the experiment. By definition, the run configuration is the combination of the training environment and the compute instance. Returns: RunConfiguration: The run configuration of the experiment. \"\"\" run_config = RunConfiguration () docker_config = DockerConfiguration ( use_docker = True ) run_config . docker = docker_config aml_run_env = Environment . get ( self . interface . workspace , self . env_name , ) run_config . environment = aml_run_env run_config . target = self . compute_target return run_config def submit_pipeline ( self , steps : List [ PythonScriptStep ]): \"\"\"Used to submit a training pipeline. Args: steps (List[PythonScriptStep]): The different steps of the training pipeline. https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb This should be a registered AZML Pipeline with the followings steps : * Download raw datas from one datastore * Transform those datas in \"gold\" datas and store them in another datastore * Use these gold datas to train a model * Evaluate that model * Save and version that model \"\"\" experiment = Experiment ( self . interface . workspace , self . experiment_name ) # src_dir = __here__ # src_dir = str(Path.cwd()) pipeline = Pipeline ( workspace = self . interface . workspace , steps = steps ) # TODO: add the validation method of the pipeline. log . info ( \"Submitting Run\" ) run = experiment . submit ( config = pipeline ) run . wait_for_completion ( show_output = True ) log . info ( \"Run completed.\" ) if self . clean_after_run : log . info ( \"Deleting compute instance.\" ) self . compute_target . delete () def submit_run ( self ): \"\"\"Submit your training loop and create an experiment. This experiment is defined by the use of the `ScriptRunConfig` class. This means that you have to provide the path to a script defining your training loop, defined by the parameters `source_directory` and `script` of the `ScriptRunConfig` class, `script` being the path of your training loop relative to `source_directory`. For example purpose, a training loop example is provided [TrainingLoopExample][azure_helper.steps.train] is provided, as well as an abstract class if you want to use this training loop structure, but you're not forced to. \"\"\" experiment = Experiment ( self . interface . workspace , self . experiment_name ) # src_dir = __here__ src_dir = str ( Path . cwd ()) docker_config = DockerConfiguration ( use_docker = True ) script = ScriptRunConfig ( source_directory = src_dir , script = self . training_script_path , docker_runtime_config = docker_config , ) script . run_config = self . generate_run_config () # script.run_config.target = self.compute_target # aml_run_env = Environment.get( # self.interface.workspace, # self.env_name, # ) # script.run_config.environment = aml_run_env log . info ( \"Submitting Run\" ) run = experiment . submit ( config = script ) run . wait_for_completion ( show_output = True ) log . info ( f \"Run completed : { run . get_metrics () } \" ) if self . clean_after_run : log . info ( \"Deleting compute instance.\" ) self . compute_target . delete ()","title":"AMLExperiment"},{"location":"steps/create_aml_experiment/#azure_helper.steps.create_aml_experiment.AMLExperiment.__init__","text":"Instantiate the creation of an AzureML Experiment needed to train a model. Parameters: Name Type Description Default aml_interface AMLInterface The aml_interface needed to register the experiment in the workspace. required aml_compute_name str The name of the compute instance used. required aml_compute_instance str The size (as in vm_size ) of the compute instance used. required min_node int The minimum number of nodes to use on the compute instance. Defaults to 1. 1 max_node int The maximum number of nodes to use on the compute instance. Defaults to 2. 2 env_name str The name of the training environment. required experiment_name str The name of the experiment. required training_script_path str The path to the training loop script. required clean_after_run bool Whether or not you want to delete the compute instance after training. Defaults to True. True Source code in azure_helper/steps/create_aml_experiment.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def __init__ ( self , aml_interface : AMLInterface , aml_compute_name : str , aml_compute_instance : str , env_name : str , experiment_name : str , training_script_path : str , min_node : int = 1 , max_node : int = 2 , clean_after_run : bool = True , ) -> None : \"\"\"Instantiate the creation of an AzureML Experiment needed to train a model. Args: aml_interface (AMLInterface): The aml_interface needed to register the experiment in the workspace. aml_compute_name (str): The name of the compute instance used. aml_compute_instance (str): The size (as in `vm_size`) of the compute instance used. min_node (int, optional): The minimum number of nodes to use on the compute instance. Defaults to 1. max_node (int, optional): The maximum number of nodes to use on the compute instance. Defaults to 2. env_name (str): The name of the training environment. experiment_name (str): The name of the experiment. training_script_path (str): The path to the training loop script. clean_after_run (bool, optional): Whether or not you want to delete the compute instance after training. Defaults to True. \"\"\" self . interface = aml_interface self . aml_compute_name = aml_compute_name self . aml_compute_instance = aml_compute_instance self . env_name = env_name self . experiment_name = experiment_name self . clean_after_run = clean_after_run self . training_script_path = training_script_path self . compute_target = aml_interface . get_compute_target ( aml_compute_name , aml_compute_instance , min_node , max_node , )","title":"__init__()"},{"location":"steps/create_aml_experiment/#azure_helper.steps.create_aml_experiment.AMLExperiment.generate_run_config","text":"Generate the run configuration of the experiment. By definition, the run configuration is the combination of the training environment and the compute instance. Returns: Name Type Description RunConfiguration RunConfiguration The run configuration of the experiment. Source code in azure_helper/steps/create_aml_experiment.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def generate_run_config ( self ) -> RunConfiguration : \"\"\"Generate the run configuration of the experiment. By definition, the run configuration is the combination of the training environment and the compute instance. Returns: RunConfiguration: The run configuration of the experiment. \"\"\" run_config = RunConfiguration () docker_config = DockerConfiguration ( use_docker = True ) run_config . docker = docker_config aml_run_env = Environment . get ( self . interface . workspace , self . env_name , ) run_config . environment = aml_run_env run_config . target = self . compute_target return run_config","title":"generate_run_config()"},{"location":"steps/create_aml_experiment/#azure_helper.steps.create_aml_experiment.AMLExperiment.submit_pipeline","text":"Used to submit a training pipeline. Parameters: Name Type Description Default steps List [ PythonScriptStep ] The different steps of the training pipeline. required This should be a registered AZML Pipeline with the followings steps : Download raw datas from one datastore Transform those datas in \"gold\" datas and store them in another datastore Use these gold datas to train a model Evaluate that model Save and version that model Source code in azure_helper/steps/create_aml_experiment.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def submit_pipeline ( self , steps : List [ PythonScriptStep ]): \"\"\"Used to submit a training pipeline. Args: steps (List[PythonScriptStep]): The different steps of the training pipeline. https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb This should be a registered AZML Pipeline with the followings steps : * Download raw datas from one datastore * Transform those datas in \"gold\" datas and store them in another datastore * Use these gold datas to train a model * Evaluate that model * Save and version that model \"\"\" experiment = Experiment ( self . interface . workspace , self . experiment_name ) # src_dir = __here__ # src_dir = str(Path.cwd()) pipeline = Pipeline ( workspace = self . interface . workspace , steps = steps ) # TODO: add the validation method of the pipeline. log . info ( \"Submitting Run\" ) run = experiment . submit ( config = pipeline ) run . wait_for_completion ( show_output = True ) log . info ( \"Run completed.\" ) if self . clean_after_run : log . info ( \"Deleting compute instance.\" ) self . compute_target . delete ()","title":"submit_pipeline()"},{"location":"steps/create_aml_experiment/#azure_helper.steps.create_aml_experiment.AMLExperiment.submit_run","text":"Submit your training loop and create an experiment. This experiment is defined by the use of the ScriptRunConfig class. This means that you have to provide the path to a script defining your training loop, defined by the parameters source_directory and script of the ScriptRunConfig class, script being the path of your training loop relative to source_directory . For example purpose, a training loop example is provided TrainingLoopExample is provided, as well as an abstract class if you want to use this training loop structure, but you're not forced to. Source code in azure_helper/steps/create_aml_experiment.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def submit_run ( self ): \"\"\"Submit your training loop and create an experiment. This experiment is defined by the use of the `ScriptRunConfig` class. This means that you have to provide the path to a script defining your training loop, defined by the parameters `source_directory` and `script` of the `ScriptRunConfig` class, `script` being the path of your training loop relative to `source_directory`. For example purpose, a training loop example is provided [TrainingLoopExample][azure_helper.steps.train] is provided, as well as an abstract class if you want to use this training loop structure, but you're not forced to. \"\"\" experiment = Experiment ( self . interface . workspace , self . experiment_name ) # src_dir = __here__ src_dir = str ( Path . cwd ()) docker_config = DockerConfiguration ( use_docker = True ) script = ScriptRunConfig ( source_directory = src_dir , script = self . training_script_path , docker_runtime_config = docker_config , ) script . run_config = self . generate_run_config () # script.run_config.target = self.compute_target # aml_run_env = Environment.get( # self.interface.workspace, # self.env_name, # ) # script.run_config.environment = aml_run_env log . info ( \"Submitting Run\" ) run = experiment . submit ( config = script ) run . wait_for_completion ( show_output = True ) log . info ( f \"Run completed : { run . get_metrics () } \" ) if self . clean_after_run : log . info ( \"Deleting compute instance.\" ) self . compute_target . delete ()","title":"submit_run()"},{"location":"steps/create_data/","text":"Data Creation steps CreateData Source code in azure_helper/steps/create_data.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 class CreateData : def __init__ ( self , project_name : str , train_datastore : str = \"train\" , test_datastore : str = \"test\" , ): \"\"\"This class is just a wrapper around the [BlobStorageInterface][azure_helper.utils.blob_storage_interface] and might disappear, as it is not really needed. Args: project_name (str): _description_ train_datastore (str, optional): _description_. Defaults to \"train\". test_datastore (str, optional): _description_. Defaults to \"test\". \"\"\" self . project_name = project_name self . train_datastore = train_datastore self . test_datastore = test_datastore def upload_training_data ( self , blob_storage_interface : BlobStorageInterface , x_train : pd . DataFrame , y_train : pd . DataFrame , ): \"\"\"Upload datas to the training blob storage. Args: blob_storage_interface (BlobStorageInterface): The interface with your storage account. x_train (pd.DataFrame): Train datas. y_train (pd.DataFrame): Train datas. \"\"\" blob_storage_interface . upload_df_to_blob ( dataframe = x_train , container_name = f \" { self . project_name } \" , blob_path = f \" { self . train_datastore } /X_train.csv\" , ) blob_storage_interface . upload_df_to_blob ( dataframe = y_train , container_name = f \" { self . project_name } \" , blob_path = f \" { self . train_datastore } /y_train.csv\" , ) def upload_validation_data ( self , blob_storage_interface : BlobStorageInterface , x_valid : pd . DataFrame , y_valid : pd . DataFrame , ): \"\"\"Upload datas to the validation blob storage. Args: blob_storage_interface (BlobStorageInterface): The interface with your storage account. x_valid (pd.DataFrame): Validation datas. y_valid (pd.DataFrame): Validation datas. \"\"\" # Data to be used during model validation blob_storage_interface . upload_df_to_blob ( dataframe = x_valid , container_name = f \" { self . project_name } \" , blob_path = f \" { self . train_datastore } /X_valid.csv\" , ) blob_storage_interface . upload_df_to_blob ( dataframe = y_valid , container_name = f \" { self . project_name } \" , blob_path = f \" { self . train_datastore } /y_valid.csv\" , ) def upload_test_data ( self , blob_storage_interface : BlobStorageInterface , x_test : pd . DataFrame , y_test : pd . DataFrame , ): \"\"\"Upload datas to the test blob storage. Args: blob_storage_interface (BlobStorageInterface): The interface with your storage account. x_test (pd.DataFrame): Test datas. y_test (pd.DataFrame): Test datas. \"\"\" # Data to be used during model evaluation # So stored in the training container blob_storage_interface . upload_df_to_blob ( dataframe = x_test , container_name = f \" { self . project_name } \" , blob_path = f \" { self . test_datastore } /X_test.csv\" , ) blob_storage_interface . upload_df_to_blob ( dataframe = y_test , container_name = f \" { self . project_name } \" , blob_path = f \" { self . test_datastore } /y_test.csv\" , ) __init__ ( project_name , train_datastore = 'train' , test_datastore = 'test' ) This class is just a wrapper around the BlobStorageInterface and might disappear, as it is not really needed. Parameters: Name Type Description Default project_name str description required train_datastore str description . Defaults to \"train\". 'train' test_datastore str description . Defaults to \"test\". 'test' Source code in azure_helper/steps/create_data.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def __init__ ( self , project_name : str , train_datastore : str = \"train\" , test_datastore : str = \"test\" , ): \"\"\"This class is just a wrapper around the [BlobStorageInterface][azure_helper.utils.blob_storage_interface] and might disappear, as it is not really needed. Args: project_name (str): _description_ train_datastore (str, optional): _description_. Defaults to \"train\". test_datastore (str, optional): _description_. Defaults to \"test\". \"\"\" self . project_name = project_name self . train_datastore = train_datastore self . test_datastore = test_datastore upload_training_data ( blob_storage_interface , x_train , y_train ) Upload datas to the training blob storage. Parameters: Name Type Description Default blob_storage_interface BlobStorageInterface The interface with your storage account. required x_train pd . DataFrame Train datas. required y_train pd . DataFrame Train datas. required Source code in azure_helper/steps/create_data.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def upload_training_data ( self , blob_storage_interface : BlobStorageInterface , x_train : pd . DataFrame , y_train : pd . DataFrame , ): \"\"\"Upload datas to the training blob storage. Args: blob_storage_interface (BlobStorageInterface): The interface with your storage account. x_train (pd.DataFrame): Train datas. y_train (pd.DataFrame): Train datas. \"\"\" blob_storage_interface . upload_df_to_blob ( dataframe = x_train , container_name = f \" { self . project_name } \" , blob_path = f \" { self . train_datastore } /X_train.csv\" , ) blob_storage_interface . upload_df_to_blob ( dataframe = y_train , container_name = f \" { self . project_name } \" , blob_path = f \" { self . train_datastore } /y_train.csv\" , ) upload_validation_data ( blob_storage_interface , x_valid , y_valid ) Upload datas to the validation blob storage. Parameters: Name Type Description Default blob_storage_interface BlobStorageInterface The interface with your storage account. required x_valid pd . DataFrame Validation datas. required y_valid pd . DataFrame Validation datas. required Source code in azure_helper/steps/create_data.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def upload_validation_data ( self , blob_storage_interface : BlobStorageInterface , x_valid : pd . DataFrame , y_valid : pd . DataFrame , ): \"\"\"Upload datas to the validation blob storage. Args: blob_storage_interface (BlobStorageInterface): The interface with your storage account. x_valid (pd.DataFrame): Validation datas. y_valid (pd.DataFrame): Validation datas. \"\"\" # Data to be used during model validation blob_storage_interface . upload_df_to_blob ( dataframe = x_valid , container_name = f \" { self . project_name } \" , blob_path = f \" { self . train_datastore } /X_valid.csv\" , ) blob_storage_interface . upload_df_to_blob ( dataframe = y_valid , container_name = f \" { self . project_name } \" , blob_path = f \" { self . train_datastore } /y_valid.csv\" , ) upload_test_data ( blob_storage_interface , x_test , y_test ) Upload datas to the test blob storage. Parameters: Name Type Description Default blob_storage_interface BlobStorageInterface The interface with your storage account. required x_test pd . DataFrame Test datas. required y_test pd . DataFrame Test datas. required Source code in azure_helper/steps/create_data.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def upload_test_data ( self , blob_storage_interface : BlobStorageInterface , x_test : pd . DataFrame , y_test : pd . DataFrame , ): \"\"\"Upload datas to the test blob storage. Args: blob_storage_interface (BlobStorageInterface): The interface with your storage account. x_test (pd.DataFrame): Test datas. y_test (pd.DataFrame): Test datas. \"\"\" # Data to be used during model evaluation # So stored in the training container blob_storage_interface . upload_df_to_blob ( dataframe = x_test , container_name = f \" { self . project_name } \" , blob_path = f \" { self . test_datastore } /X_test.csv\" , ) blob_storage_interface . upload_df_to_blob ( dataframe = y_test , container_name = f \" { self . project_name } \" , blob_path = f \" { self . test_datastore } /y_test.csv\" , )","title":"Data creation"},{"location":"steps/create_data/#data-creation-steps","text":"","title":"Data Creation steps"},{"location":"steps/create_data/#azure_helper.steps.create_data.CreateData","text":"Source code in azure_helper/steps/create_data.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 class CreateData : def __init__ ( self , project_name : str , train_datastore : str = \"train\" , test_datastore : str = \"test\" , ): \"\"\"This class is just a wrapper around the [BlobStorageInterface][azure_helper.utils.blob_storage_interface] and might disappear, as it is not really needed. Args: project_name (str): _description_ train_datastore (str, optional): _description_. Defaults to \"train\". test_datastore (str, optional): _description_. Defaults to \"test\". \"\"\" self . project_name = project_name self . train_datastore = train_datastore self . test_datastore = test_datastore def upload_training_data ( self , blob_storage_interface : BlobStorageInterface , x_train : pd . DataFrame , y_train : pd . DataFrame , ): \"\"\"Upload datas to the training blob storage. Args: blob_storage_interface (BlobStorageInterface): The interface with your storage account. x_train (pd.DataFrame): Train datas. y_train (pd.DataFrame): Train datas. \"\"\" blob_storage_interface . upload_df_to_blob ( dataframe = x_train , container_name = f \" { self . project_name } \" , blob_path = f \" { self . train_datastore } /X_train.csv\" , ) blob_storage_interface . upload_df_to_blob ( dataframe = y_train , container_name = f \" { self . project_name } \" , blob_path = f \" { self . train_datastore } /y_train.csv\" , ) def upload_validation_data ( self , blob_storage_interface : BlobStorageInterface , x_valid : pd . DataFrame , y_valid : pd . DataFrame , ): \"\"\"Upload datas to the validation blob storage. Args: blob_storage_interface (BlobStorageInterface): The interface with your storage account. x_valid (pd.DataFrame): Validation datas. y_valid (pd.DataFrame): Validation datas. \"\"\" # Data to be used during model validation blob_storage_interface . upload_df_to_blob ( dataframe = x_valid , container_name = f \" { self . project_name } \" , blob_path = f \" { self . train_datastore } /X_valid.csv\" , ) blob_storage_interface . upload_df_to_blob ( dataframe = y_valid , container_name = f \" { self . project_name } \" , blob_path = f \" { self . train_datastore } /y_valid.csv\" , ) def upload_test_data ( self , blob_storage_interface : BlobStorageInterface , x_test : pd . DataFrame , y_test : pd . DataFrame , ): \"\"\"Upload datas to the test blob storage. Args: blob_storage_interface (BlobStorageInterface): The interface with your storage account. x_test (pd.DataFrame): Test datas. y_test (pd.DataFrame): Test datas. \"\"\" # Data to be used during model evaluation # So stored in the training container blob_storage_interface . upload_df_to_blob ( dataframe = x_test , container_name = f \" { self . project_name } \" , blob_path = f \" { self . test_datastore } /X_test.csv\" , ) blob_storage_interface . upload_df_to_blob ( dataframe = y_test , container_name = f \" { self . project_name } \" , blob_path = f \" { self . test_datastore } /y_test.csv\" , )","title":"CreateData"},{"location":"steps/create_data/#azure_helper.steps.create_data.CreateData.__init__","text":"This class is just a wrapper around the BlobStorageInterface and might disappear, as it is not really needed. Parameters: Name Type Description Default project_name str description required train_datastore str description . Defaults to \"train\". 'train' test_datastore str description . Defaults to \"test\". 'test' Source code in azure_helper/steps/create_data.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def __init__ ( self , project_name : str , train_datastore : str = \"train\" , test_datastore : str = \"test\" , ): \"\"\"This class is just a wrapper around the [BlobStorageInterface][azure_helper.utils.blob_storage_interface] and might disappear, as it is not really needed. Args: project_name (str): _description_ train_datastore (str, optional): _description_. Defaults to \"train\". test_datastore (str, optional): _description_. Defaults to \"test\". \"\"\" self . project_name = project_name self . train_datastore = train_datastore self . test_datastore = test_datastore","title":"__init__()"},{"location":"steps/create_data/#azure_helper.steps.create_data.CreateData.upload_training_data","text":"Upload datas to the training blob storage. Parameters: Name Type Description Default blob_storage_interface BlobStorageInterface The interface with your storage account. required x_train pd . DataFrame Train datas. required y_train pd . DataFrame Train datas. required Source code in azure_helper/steps/create_data.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def upload_training_data ( self , blob_storage_interface : BlobStorageInterface , x_train : pd . DataFrame , y_train : pd . DataFrame , ): \"\"\"Upload datas to the training blob storage. Args: blob_storage_interface (BlobStorageInterface): The interface with your storage account. x_train (pd.DataFrame): Train datas. y_train (pd.DataFrame): Train datas. \"\"\" blob_storage_interface . upload_df_to_blob ( dataframe = x_train , container_name = f \" { self . project_name } \" , blob_path = f \" { self . train_datastore } /X_train.csv\" , ) blob_storage_interface . upload_df_to_blob ( dataframe = y_train , container_name = f \" { self . project_name } \" , blob_path = f \" { self . train_datastore } /y_train.csv\" , )","title":"upload_training_data()"},{"location":"steps/create_data/#azure_helper.steps.create_data.CreateData.upload_validation_data","text":"Upload datas to the validation blob storage. Parameters: Name Type Description Default blob_storage_interface BlobStorageInterface The interface with your storage account. required x_valid pd . DataFrame Validation datas. required y_valid pd . DataFrame Validation datas. required Source code in azure_helper/steps/create_data.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def upload_validation_data ( self , blob_storage_interface : BlobStorageInterface , x_valid : pd . DataFrame , y_valid : pd . DataFrame , ): \"\"\"Upload datas to the validation blob storage. Args: blob_storage_interface (BlobStorageInterface): The interface with your storage account. x_valid (pd.DataFrame): Validation datas. y_valid (pd.DataFrame): Validation datas. \"\"\" # Data to be used during model validation blob_storage_interface . upload_df_to_blob ( dataframe = x_valid , container_name = f \" { self . project_name } \" , blob_path = f \" { self . train_datastore } /X_valid.csv\" , ) blob_storage_interface . upload_df_to_blob ( dataframe = y_valid , container_name = f \" { self . project_name } \" , blob_path = f \" { self . train_datastore } /y_valid.csv\" , )","title":"upload_validation_data()"},{"location":"steps/create_data/#azure_helper.steps.create_data.CreateData.upload_test_data","text":"Upload datas to the test blob storage. Parameters: Name Type Description Default blob_storage_interface BlobStorageInterface The interface with your storage account. required x_test pd . DataFrame Test datas. required y_test pd . DataFrame Test datas. required Source code in azure_helper/steps/create_data.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def upload_test_data ( self , blob_storage_interface : BlobStorageInterface , x_test : pd . DataFrame , y_test : pd . DataFrame , ): \"\"\"Upload datas to the test blob storage. Args: blob_storage_interface (BlobStorageInterface): The interface with your storage account. x_test (pd.DataFrame): Test datas. y_test (pd.DataFrame): Test datas. \"\"\" # Data to be used during model evaluation # So stored in the training container blob_storage_interface . upload_df_to_blob ( dataframe = x_test , container_name = f \" { self . project_name } \" , blob_path = f \" { self . test_datastore } /X_test.csv\" , ) blob_storage_interface . upload_df_to_blob ( dataframe = y_test , container_name = f \" { self . project_name } \" , blob_path = f \" { self . test_datastore } /y_test.csv\" , )","title":"upload_test_data()"},{"location":"steps/deploy_aml_model/","text":"The Model Deployment azure_helper.steps.deploy_aml_model DeploymentSettings Bases: BaseModel Basic settings needed for the deployment, whether it is with ACI or AKS. Parameters: Name Type Description Default deployment_service_name str the name of the service you want to deploy or update. required cpu_cores int The number of cpu cores needed. Defaults to 1. required gpu_cores int The number of gpu cores needed. Defaults to 0. required memory_gb int The memory in gb needed. Defaults to 1. required enable_app_insights bool Enable app insights monitoring. Defaults to True. required Source code in azure_helper/steps/deploy_aml_model.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 class DeploymentSettings ( BaseModel ): \"\"\"Basic settings needed for the deployment, whether it is with ACI or AKS. Args: deployment_service_name (str): the name of the service you want to deploy or update. cpu_cores (int): The number of cpu cores needed. Defaults to 1. gpu_cores (int): The number of gpu cores needed. Defaults to 0. memory_gb (int): The memory in gb needed. Defaults to 1. enable_app_insights (bool): Enable app insights monitoring. Defaults to True. \"\"\" deployment_service_name : str cpu_cores : int = 1 gpu_cores : int = 0 memory_gb : int = 1 enable_app_insights : bool = True DeployModel Source code in azure_helper/steps/deploy_aml_model.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 class DeployModel : def __init__ ( self , aml_interface : AMLInterface , aml_env_name : str , model_name : str , script_config_path : Path , deployment_settings : DeploymentSettings , ) -> None : \"\"\"Instantiate the deployment of the model. Args: aml_interface (AMLInterface): The AMLInterface which will be responsible to deploy the model. aml_env_name (str): The name of the AMLEnvironment you will use to deploy the model. It is not necessarily the same used to train the model. model_name (str): The name of the model you deploy. script_config_path (Path): The location of the inference script. deployment_settings (DeploymentSettings): the basic settings of the deployment. \"\"\" self . aml_interface = aml_interface self . workspace = aml_interface . workspace self . aml_env_name = aml_env_name self . model_name = model_name self . script_config_path = script_config_path self . deployment_settings = deployment_settings def get_inference_config ( self , ) -> InferenceConfig : \"\"\"Fetch the inference script config needed to interact the endpoint deployed. Returns: InferenceConfig: The instantiated inference config. \"\"\" aml_env = Environment . get ( workspace = self . workspace , name = self . aml_env_name , ) # scoring_script_path = os.path.join(__here__, \"score.py\") scoring_script_path = str ( self . script_config_path ) return InferenceConfig ( entry_script = scoring_script_path , environment = aml_env , ) def deploy_aciservice ( self , * args , ** kwargs , ): \"\"\"Deploy an ACI service to serve the model.\"\"\" inference_config = self . get_inference_config () aci_deployment = AciWebservice . deploy_configuration ( * args , ** kwargs , cpu_cores = self . deployment_settings . cpu_cores , memory_gb = self . deployment_settings . memory_gb , enable_app_insights = self . deployment_settings . enable_app_insights , ) model = self . workspace . models . get ( self . model_name ) service = Model . deploy ( workspace = self . workspace , name = self . deployment_settings . deployment_service_name , models = [ model ], inference_config = inference_config , deployment_config = aci_deployment , ) service . wait_for_deployment ( show_output = True ) log . info ( service . state ) log . info ( service . scoring_uri ) def deploy_aksservice ( self , aks_cluster_name : str , * args , ** kwargs , ): \"\"\"Deploy an AKS service to serve the model. Args: script_config_path (Path): The location of the script for the inference config. aks_cluster_name (str): The name of the k8s cluster on which you want to deploy. Contrary to an ACI deployment, you need a pre-existing k8s cluster in your workspace to use AKS deployment. \"\"\" # Verify that cluster does not exist already try : aks_target = ComputeTarget ( self . workspace , name = aks_cluster_name ) log . info ( f \"k8s cluster { aks_cluster_name } found in workspace { self . workspace } \" , ) except ComputeTargetException : log . warning ( f \"k8s cluster { aks_cluster_name } was not found in workspace { self . workspace } . Now provisioning one.\" , ) # Use the default configuration (can also provide parameters to customize) provisioning_config = AksCompute . provisioning_configuration () # Create the cluster aks_target = ComputeTarget . create ( workspace = self . workspace , name = aks_cluster_name , provisioning_configuration = provisioning_config , ) aks_target . wait_for_completion ( show_output = True , timeout_in_minutes = 10 , ) inference_config = self . get_inference_config () aks_deployment = AksWebservice . deploy_configuration ( * args , ** kwargs , cpu_cores = self . deployment_settings . cpu_cores , memory_gb = self . deployment_settings . memory_gb , enable_app_insights = self . deployment_settings . enable_app_insights , ) model = self . workspace . models . get ( self . model_name ) service = Model . deploy ( workspace = self . workspace , name = self . deployment_settings . deployment_service_name , models = [ model ], inference_config = inference_config , deployment_config = aks_deployment , deployment_target = aks_target , ) service . wait_for_deployment ( show_output = True ) log . info ( service . state ) log . info ( service . scoring_uri ) def update_service ( self , ): \"\"\"Update an already existing service, ACI or AKS.\"\"\" inference_config = self . get_inference_config () service = Webservice ( name = self . deployment_settings . deployment_service_name , workspace = self . workspace , ) model = self . workspace . models . get ( self . model_name ) service . update ( models = [ model ], inference_config = inference_config ) log . info ( service . state ) log . info ( service . scoring_uri ) __init__ ( aml_interface , aml_env_name , model_name , script_config_path , deployment_settings ) Instantiate the deployment of the model. Parameters: Name Type Description Default aml_interface AMLInterface The AMLInterface which will be responsible to deploy the model. required aml_env_name str The name of the AMLEnvironment you will use to deploy the model. It is not necessarily the same used to train the model. required model_name str The name of the model you deploy. required script_config_path Path The location of the inference script. required deployment_settings DeploymentSettings the basic settings of the deployment. required Source code in azure_helper/steps/deploy_aml_model.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def __init__ ( self , aml_interface : AMLInterface , aml_env_name : str , model_name : str , script_config_path : Path , deployment_settings : DeploymentSettings , ) -> None : \"\"\"Instantiate the deployment of the model. Args: aml_interface (AMLInterface): The AMLInterface which will be responsible to deploy the model. aml_env_name (str): The name of the AMLEnvironment you will use to deploy the model. It is not necessarily the same used to train the model. model_name (str): The name of the model you deploy. script_config_path (Path): The location of the inference script. deployment_settings (DeploymentSettings): the basic settings of the deployment. \"\"\" self . aml_interface = aml_interface self . workspace = aml_interface . workspace self . aml_env_name = aml_env_name self . model_name = model_name self . script_config_path = script_config_path self . deployment_settings = deployment_settings get_inference_config () Fetch the inference script config needed to interact the endpoint deployed. Returns: Name Type Description InferenceConfig InferenceConfig The instantiated inference config. Source code in azure_helper/steps/deploy_aml_model.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def get_inference_config ( self , ) -> InferenceConfig : \"\"\"Fetch the inference script config needed to interact the endpoint deployed. Returns: InferenceConfig: The instantiated inference config. \"\"\" aml_env = Environment . get ( workspace = self . workspace , name = self . aml_env_name , ) # scoring_script_path = os.path.join(__here__, \"score.py\") scoring_script_path = str ( self . script_config_path ) return InferenceConfig ( entry_script = scoring_script_path , environment = aml_env , ) deploy_aciservice ( * args , ** kwargs ) Deploy an ACI service to serve the model. Source code in azure_helper/steps/deploy_aml_model.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def deploy_aciservice ( self , * args , ** kwargs , ): \"\"\"Deploy an ACI service to serve the model.\"\"\" inference_config = self . get_inference_config () aci_deployment = AciWebservice . deploy_configuration ( * args , ** kwargs , cpu_cores = self . deployment_settings . cpu_cores , memory_gb = self . deployment_settings . memory_gb , enable_app_insights = self . deployment_settings . enable_app_insights , ) model = self . workspace . models . get ( self . model_name ) service = Model . deploy ( workspace = self . workspace , name = self . deployment_settings . deployment_service_name , models = [ model ], inference_config = inference_config , deployment_config = aci_deployment , ) service . wait_for_deployment ( show_output = True ) log . info ( service . state ) log . info ( service . scoring_uri ) deploy_aksservice ( aks_cluster_name , * args , ** kwargs ) Deploy an AKS service to serve the model. Parameters: Name Type Description Default script_config_path Path The location of the script for the inference config. required aks_cluster_name str The name of the k8s cluster on which you want to deploy. Contrary to an ACI deployment, you need a pre-existing k8s cluster in your workspace to use AKS deployment. required Source code in azure_helper/steps/deploy_aml_model.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 def deploy_aksservice ( self , aks_cluster_name : str , * args , ** kwargs , ): \"\"\"Deploy an AKS service to serve the model. Args: script_config_path (Path): The location of the script for the inference config. aks_cluster_name (str): The name of the k8s cluster on which you want to deploy. Contrary to an ACI deployment, you need a pre-existing k8s cluster in your workspace to use AKS deployment. \"\"\" # Verify that cluster does not exist already try : aks_target = ComputeTarget ( self . workspace , name = aks_cluster_name ) log . info ( f \"k8s cluster { aks_cluster_name } found in workspace { self . workspace } \" , ) except ComputeTargetException : log . warning ( f \"k8s cluster { aks_cluster_name } was not found in workspace { self . workspace } . Now provisioning one.\" , ) # Use the default configuration (can also provide parameters to customize) provisioning_config = AksCompute . provisioning_configuration () # Create the cluster aks_target = ComputeTarget . create ( workspace = self . workspace , name = aks_cluster_name , provisioning_configuration = provisioning_config , ) aks_target . wait_for_completion ( show_output = True , timeout_in_minutes = 10 , ) inference_config = self . get_inference_config () aks_deployment = AksWebservice . deploy_configuration ( * args , ** kwargs , cpu_cores = self . deployment_settings . cpu_cores , memory_gb = self . deployment_settings . memory_gb , enable_app_insights = self . deployment_settings . enable_app_insights , ) model = self . workspace . models . get ( self . model_name ) service = Model . deploy ( workspace = self . workspace , name = self . deployment_settings . deployment_service_name , models = [ model ], inference_config = inference_config , deployment_config = aks_deployment , deployment_target = aks_target , ) service . wait_for_deployment ( show_output = True ) log . info ( service . state ) log . info ( service . scoring_uri ) update_service () Update an already existing service, ACI or AKS. Source code in azure_helper/steps/deploy_aml_model.py 177 178 179 180 181 182 183 184 185 186 187 188 189 def update_service ( self , ): \"\"\"Update an already existing service, ACI or AKS.\"\"\" inference_config = self . get_inference_config () service = Webservice ( name = self . deployment_settings . deployment_service_name , workspace = self . workspace , ) model = self . workspace . models . get ( self . model_name ) service . update ( models = [ model ], inference_config = inference_config ) log . info ( service . state ) log . info ( service . scoring_uri ) Deploy to AKS Web service authentication When deploying to Azure Kubernetes Service, key-based authentication is enabled by default . You can also enable token-based authentication. Token-based authentication requires clients to use an Azure Active Directory account to request an authentication token, which is used to make requests to the deployed service. Limitations Using a service principal with AKS is not supported by Azure Machine Learning . The AKS cluster must use a system-assigned managed identity instead. Tutorial Prepare an application for Azure Kubernetes Service (AKS) Create a Kubernetes cluster 1 2 3 4 5 6 az aks create \\ --resource-group myResourceGroup \\ --name myAKSCluster \\ --node-count 2 \\ --generate-ssh-keys \\ --attach-acr <acrName> Install kubectl 1 az aks install-cli Connect to cluster using kubectl 1 az aks get-credentials --resource-group myResourceGroup --name myAKSCluster Update the manifest file In these tutorials, an Azure Container Registry (ACR) instance stores the container image for the sample application. To deploy the application, you must update the image name in the Kubernetes manifest file to include the ACR login server name. Deploy the application To deploy your application, use the kubectl apply command. 1 kubectl apply -f manifest.yaml","title":"Deployment class"},{"location":"steps/deploy_aml_model/#the-model-deployment","text":"","title":"The Model Deployment"},{"location":"steps/deploy_aml_model/#azure_helper.steps.deploy_aml_model","text":"","title":"deploy_aml_model"},{"location":"steps/deploy_aml_model/#azure_helper.steps.deploy_aml_model.DeploymentSettings","text":"Bases: BaseModel Basic settings needed for the deployment, whether it is with ACI or AKS. Parameters: Name Type Description Default deployment_service_name str the name of the service you want to deploy or update. required cpu_cores int The number of cpu cores needed. Defaults to 1. required gpu_cores int The number of gpu cores needed. Defaults to 0. required memory_gb int The memory in gb needed. Defaults to 1. required enable_app_insights bool Enable app insights monitoring. Defaults to True. required Source code in azure_helper/steps/deploy_aml_model.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 class DeploymentSettings ( BaseModel ): \"\"\"Basic settings needed for the deployment, whether it is with ACI or AKS. Args: deployment_service_name (str): the name of the service you want to deploy or update. cpu_cores (int): The number of cpu cores needed. Defaults to 1. gpu_cores (int): The number of gpu cores needed. Defaults to 0. memory_gb (int): The memory in gb needed. Defaults to 1. enable_app_insights (bool): Enable app insights monitoring. Defaults to True. \"\"\" deployment_service_name : str cpu_cores : int = 1 gpu_cores : int = 0 memory_gb : int = 1 enable_app_insights : bool = True","title":"DeploymentSettings"},{"location":"steps/deploy_aml_model/#azure_helper.steps.deploy_aml_model.DeployModel","text":"Source code in azure_helper/steps/deploy_aml_model.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 class DeployModel : def __init__ ( self , aml_interface : AMLInterface , aml_env_name : str , model_name : str , script_config_path : Path , deployment_settings : DeploymentSettings , ) -> None : \"\"\"Instantiate the deployment of the model. Args: aml_interface (AMLInterface): The AMLInterface which will be responsible to deploy the model. aml_env_name (str): The name of the AMLEnvironment you will use to deploy the model. It is not necessarily the same used to train the model. model_name (str): The name of the model you deploy. script_config_path (Path): The location of the inference script. deployment_settings (DeploymentSettings): the basic settings of the deployment. \"\"\" self . aml_interface = aml_interface self . workspace = aml_interface . workspace self . aml_env_name = aml_env_name self . model_name = model_name self . script_config_path = script_config_path self . deployment_settings = deployment_settings def get_inference_config ( self , ) -> InferenceConfig : \"\"\"Fetch the inference script config needed to interact the endpoint deployed. Returns: InferenceConfig: The instantiated inference config. \"\"\" aml_env = Environment . get ( workspace = self . workspace , name = self . aml_env_name , ) # scoring_script_path = os.path.join(__here__, \"score.py\") scoring_script_path = str ( self . script_config_path ) return InferenceConfig ( entry_script = scoring_script_path , environment = aml_env , ) def deploy_aciservice ( self , * args , ** kwargs , ): \"\"\"Deploy an ACI service to serve the model.\"\"\" inference_config = self . get_inference_config () aci_deployment = AciWebservice . deploy_configuration ( * args , ** kwargs , cpu_cores = self . deployment_settings . cpu_cores , memory_gb = self . deployment_settings . memory_gb , enable_app_insights = self . deployment_settings . enable_app_insights , ) model = self . workspace . models . get ( self . model_name ) service = Model . deploy ( workspace = self . workspace , name = self . deployment_settings . deployment_service_name , models = [ model ], inference_config = inference_config , deployment_config = aci_deployment , ) service . wait_for_deployment ( show_output = True ) log . info ( service . state ) log . info ( service . scoring_uri ) def deploy_aksservice ( self , aks_cluster_name : str , * args , ** kwargs , ): \"\"\"Deploy an AKS service to serve the model. Args: script_config_path (Path): The location of the script for the inference config. aks_cluster_name (str): The name of the k8s cluster on which you want to deploy. Contrary to an ACI deployment, you need a pre-existing k8s cluster in your workspace to use AKS deployment. \"\"\" # Verify that cluster does not exist already try : aks_target = ComputeTarget ( self . workspace , name = aks_cluster_name ) log . info ( f \"k8s cluster { aks_cluster_name } found in workspace { self . workspace } \" , ) except ComputeTargetException : log . warning ( f \"k8s cluster { aks_cluster_name } was not found in workspace { self . workspace } . Now provisioning one.\" , ) # Use the default configuration (can also provide parameters to customize) provisioning_config = AksCompute . provisioning_configuration () # Create the cluster aks_target = ComputeTarget . create ( workspace = self . workspace , name = aks_cluster_name , provisioning_configuration = provisioning_config , ) aks_target . wait_for_completion ( show_output = True , timeout_in_minutes = 10 , ) inference_config = self . get_inference_config () aks_deployment = AksWebservice . deploy_configuration ( * args , ** kwargs , cpu_cores = self . deployment_settings . cpu_cores , memory_gb = self . deployment_settings . memory_gb , enable_app_insights = self . deployment_settings . enable_app_insights , ) model = self . workspace . models . get ( self . model_name ) service = Model . deploy ( workspace = self . workspace , name = self . deployment_settings . deployment_service_name , models = [ model ], inference_config = inference_config , deployment_config = aks_deployment , deployment_target = aks_target , ) service . wait_for_deployment ( show_output = True ) log . info ( service . state ) log . info ( service . scoring_uri ) def update_service ( self , ): \"\"\"Update an already existing service, ACI or AKS.\"\"\" inference_config = self . get_inference_config () service = Webservice ( name = self . deployment_settings . deployment_service_name , workspace = self . workspace , ) model = self . workspace . models . get ( self . model_name ) service . update ( models = [ model ], inference_config = inference_config ) log . info ( service . state ) log . info ( service . scoring_uri )","title":"DeployModel"},{"location":"steps/deploy_aml_model/#azure_helper.steps.deploy_aml_model.DeployModel.__init__","text":"Instantiate the deployment of the model. Parameters: Name Type Description Default aml_interface AMLInterface The AMLInterface which will be responsible to deploy the model. required aml_env_name str The name of the AMLEnvironment you will use to deploy the model. It is not necessarily the same used to train the model. required model_name str The name of the model you deploy. required script_config_path Path The location of the inference script. required deployment_settings DeploymentSettings the basic settings of the deployment. required Source code in azure_helper/steps/deploy_aml_model.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def __init__ ( self , aml_interface : AMLInterface , aml_env_name : str , model_name : str , script_config_path : Path , deployment_settings : DeploymentSettings , ) -> None : \"\"\"Instantiate the deployment of the model. Args: aml_interface (AMLInterface): The AMLInterface which will be responsible to deploy the model. aml_env_name (str): The name of the AMLEnvironment you will use to deploy the model. It is not necessarily the same used to train the model. model_name (str): The name of the model you deploy. script_config_path (Path): The location of the inference script. deployment_settings (DeploymentSettings): the basic settings of the deployment. \"\"\" self . aml_interface = aml_interface self . workspace = aml_interface . workspace self . aml_env_name = aml_env_name self . model_name = model_name self . script_config_path = script_config_path self . deployment_settings = deployment_settings","title":"__init__()"},{"location":"steps/deploy_aml_model/#azure_helper.steps.deploy_aml_model.DeployModel.get_inference_config","text":"Fetch the inference script config needed to interact the endpoint deployed. Returns: Name Type Description InferenceConfig InferenceConfig The instantiated inference config. Source code in azure_helper/steps/deploy_aml_model.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def get_inference_config ( self , ) -> InferenceConfig : \"\"\"Fetch the inference script config needed to interact the endpoint deployed. Returns: InferenceConfig: The instantiated inference config. \"\"\" aml_env = Environment . get ( workspace = self . workspace , name = self . aml_env_name , ) # scoring_script_path = os.path.join(__here__, \"score.py\") scoring_script_path = str ( self . script_config_path ) return InferenceConfig ( entry_script = scoring_script_path , environment = aml_env , )","title":"get_inference_config()"},{"location":"steps/deploy_aml_model/#azure_helper.steps.deploy_aml_model.DeployModel.deploy_aciservice","text":"Deploy an ACI service to serve the model. Source code in azure_helper/steps/deploy_aml_model.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def deploy_aciservice ( self , * args , ** kwargs , ): \"\"\"Deploy an ACI service to serve the model.\"\"\" inference_config = self . get_inference_config () aci_deployment = AciWebservice . deploy_configuration ( * args , ** kwargs , cpu_cores = self . deployment_settings . cpu_cores , memory_gb = self . deployment_settings . memory_gb , enable_app_insights = self . deployment_settings . enable_app_insights , ) model = self . workspace . models . get ( self . model_name ) service = Model . deploy ( workspace = self . workspace , name = self . deployment_settings . deployment_service_name , models = [ model ], inference_config = inference_config , deployment_config = aci_deployment , ) service . wait_for_deployment ( show_output = True ) log . info ( service . state ) log . info ( service . scoring_uri )","title":"deploy_aciservice()"},{"location":"steps/deploy_aml_model/#azure_helper.steps.deploy_aml_model.DeployModel.deploy_aksservice","text":"Deploy an AKS service to serve the model. Parameters: Name Type Description Default script_config_path Path The location of the script for the inference config. required aks_cluster_name str The name of the k8s cluster on which you want to deploy. Contrary to an ACI deployment, you need a pre-existing k8s cluster in your workspace to use AKS deployment. required Source code in azure_helper/steps/deploy_aml_model.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 def deploy_aksservice ( self , aks_cluster_name : str , * args , ** kwargs , ): \"\"\"Deploy an AKS service to serve the model. Args: script_config_path (Path): The location of the script for the inference config. aks_cluster_name (str): The name of the k8s cluster on which you want to deploy. Contrary to an ACI deployment, you need a pre-existing k8s cluster in your workspace to use AKS deployment. \"\"\" # Verify that cluster does not exist already try : aks_target = ComputeTarget ( self . workspace , name = aks_cluster_name ) log . info ( f \"k8s cluster { aks_cluster_name } found in workspace { self . workspace } \" , ) except ComputeTargetException : log . warning ( f \"k8s cluster { aks_cluster_name } was not found in workspace { self . workspace } . Now provisioning one.\" , ) # Use the default configuration (can also provide parameters to customize) provisioning_config = AksCompute . provisioning_configuration () # Create the cluster aks_target = ComputeTarget . create ( workspace = self . workspace , name = aks_cluster_name , provisioning_configuration = provisioning_config , ) aks_target . wait_for_completion ( show_output = True , timeout_in_minutes = 10 , ) inference_config = self . get_inference_config () aks_deployment = AksWebservice . deploy_configuration ( * args , ** kwargs , cpu_cores = self . deployment_settings . cpu_cores , memory_gb = self . deployment_settings . memory_gb , enable_app_insights = self . deployment_settings . enable_app_insights , ) model = self . workspace . models . get ( self . model_name ) service = Model . deploy ( workspace = self . workspace , name = self . deployment_settings . deployment_service_name , models = [ model ], inference_config = inference_config , deployment_config = aks_deployment , deployment_target = aks_target , ) service . wait_for_deployment ( show_output = True ) log . info ( service . state ) log . info ( service . scoring_uri )","title":"deploy_aksservice()"},{"location":"steps/deploy_aml_model/#azure_helper.steps.deploy_aml_model.DeployModel.update_service","text":"Update an already existing service, ACI or AKS. Source code in azure_helper/steps/deploy_aml_model.py 177 178 179 180 181 182 183 184 185 186 187 188 189 def update_service ( self , ): \"\"\"Update an already existing service, ACI or AKS.\"\"\" inference_config = self . get_inference_config () service = Webservice ( name = self . deployment_settings . deployment_service_name , workspace = self . workspace , ) model = self . workspace . models . get ( self . model_name ) service . update ( models = [ model ], inference_config = inference_config ) log . info ( service . state ) log . info ( service . scoring_uri )","title":"update_service()"},{"location":"steps/deploy_aml_model/#deploy-to-aks","text":"Web service authentication When deploying to Azure Kubernetes Service, key-based authentication is enabled by default . You can also enable token-based authentication. Token-based authentication requires clients to use an Azure Active Directory account to request an authentication token, which is used to make requests to the deployed service.","title":"Deploy to AKS"},{"location":"steps/deploy_aml_model/#limitations","text":"Using a service principal with AKS is not supported by Azure Machine Learning . The AKS cluster must use a system-assigned managed identity instead.","title":"Limitations"},{"location":"steps/deploy_aml_model/#tutorial","text":"Prepare an application for Azure Kubernetes Service (AKS)","title":"Tutorial"},{"location":"steps/deploy_aml_model/#create-a-kubernetes-cluster","text":"1 2 3 4 5 6 az aks create \\ --resource-group myResourceGroup \\ --name myAKSCluster \\ --node-count 2 \\ --generate-ssh-keys \\ --attach-acr <acrName>","title":"Create a Kubernetes cluster"},{"location":"steps/deploy_aml_model/#install-kubectl","text":"1 az aks install-cli","title":"Install kubectl"},{"location":"steps/deploy_aml_model/#connect-to-cluster-using-kubectl","text":"1 az aks get-credentials --resource-group myResourceGroup --name myAKSCluster","title":"Connect to cluster using kubectl"},{"location":"steps/deploy_aml_model/#update-the-manifest-file","text":"In these tutorials, an Azure Container Registry (ACR) instance stores the container image for the sample application. To deploy the application, you must update the image name in the Kubernetes manifest file to include the ACR login server name.","title":"Update the manifest file"},{"location":"steps/deploy_aml_model/#deploy-the-application","text":"To deploy your application, use the kubectl apply command. 1 kubectl apply -f manifest.yaml","title":"Deploy the application"},{"location":"steps/inference_config/","text":"Inference Configuration azure_helper.steps.score init() and run() are both reserved functions with reserved variables needed to deploy a model, whether it is on ACI or AKS. init() defines how to load the model. run() defines how to handle the datas that are fed to the model from the REST endpoint. init () summary Source code in azure_helper/steps/score.py 15 16 17 18 19 def init (): \"\"\"_summary_\"\"\" global model model_path = Model . get_model_path ( \"MODEL_NAME\" ) model = joblib . load ( model_path ) run ( data ) summary Parameters: Name Type Description Default data _type_ description required Returns: Name Type Description _type_ description Source code in azure_helper/steps/score.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def run ( data ): \"\"\"_summary_ Args: data (_type_): _description_ Returns: _type_: _description_ \"\"\" try : data = json . loads ( data ) data = data [ \"data\" ] result = model . predict ( np . array ( data )) return result . tolist () except Exception as e : error = str ( e ) return error","title":"Inference config"},{"location":"steps/inference_config/#inference-configuration","text":"","title":"Inference Configuration"},{"location":"steps/inference_config/#azure_helper.steps.score","text":"init() and run() are both reserved functions with reserved variables needed to deploy a model, whether it is on ACI or AKS. init() defines how to load the model. run() defines how to handle the datas that are fed to the model from the REST endpoint.","title":"score"},{"location":"steps/inference_config/#azure_helper.steps.score.init","text":"summary Source code in azure_helper/steps/score.py 15 16 17 18 19 def init (): \"\"\"_summary_\"\"\" global model model_path = Model . get_model_path ( \"MODEL_NAME\" ) model = joblib . load ( model_path )","title":"init()"},{"location":"steps/inference_config/#azure_helper.steps.score.run","text":"summary Parameters: Name Type Description Default data _type_ description required Returns: Name Type Description _type_ description Source code in azure_helper/steps/score.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def run ( data ): \"\"\"_summary_ Args: data (_type_): _description_ Returns: _type_: _description_ \"\"\" try : data = json . loads ( data ) data = data [ \"data\" ] result = model . predict ( np . array ( data )) return result . tolist () except Exception as e : error = str ( e ) return error","title":"run()"},{"location":"steps/rationale/","text":"The various steps needed to train a model on Azure. We can divide the steps needed to train and deploy in the following order : Data creation/registration : have some datas available in an Azure Blob Storage. Environment creation : design and build the training environment as a Docker image. Experiment creation : attach the datas and the environment to a provisioned compute instance and build the training loop or the training pipeline. Register the resulting model. Deploy the model : For a simple model, deploy it to to a container instance or a Kubernetes cluster, and monitor it. Remark The last point is only valid for simple task if youre model is a for example an ensemble model, or must be deployed in an API. You might to do some work by hand before automation. The idea being that each of theses steps if defined by a class of this package, interacting with the AMLInterface and each others. Diagram of dependances between the classes","title":"Various steps"},{"location":"steps/rationale/#the-various-steps-needed-to-train-a-model-on-azure","text":"We can divide the steps needed to train and deploy in the following order : Data creation/registration : have some datas available in an Azure Blob Storage. Environment creation : design and build the training environment as a Docker image. Experiment creation : attach the datas and the environment to a provisioned compute instance and build the training loop or the training pipeline. Register the resulting model. Deploy the model : For a simple model, deploy it to to a container instance or a Kubernetes cluster, and monitor it. Remark The last point is only valid for simple task if youre model is a for example an ensemble model, or must be deployed in an API. You might to do some work by hand before automation. The idea being that each of theses steps if defined by a class of this package, interacting with the AMLInterface and each others.","title":"The various steps needed to train a model on Azure."},{"location":"steps/rationale/#diagram-of-dependances-between-the-classes","text":"","title":"Diagram of dependances between the classes"},{"location":"steps/train/","text":"Training Loop Example Train Bases: ABC Abstract class defining what should be the major steps of a training loop. Parameters: Name Type Description Default ABC class Abstract Class required Source code in azure_helper/steps/train.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 class Train ( ABC ): \"\"\"Abstract class defining what should be the major steps of a training loop. Args: ABC (class): Abstract Class \"\"\" @abstractmethod def get_df_from_datastore_path ( self , datastore , datastore_path ): log . info ( f \"Loading dataset { datastore_path } from datastore { datastore . name } \" ) pass @abstractmethod def prepare_data ( self ): pass @abstractmethod def train_model ( self ): log . info ( \"Start training model.\" ) pass @abstractmethod def evaluate_model ( self ): log . info ( \"Start evaluating model.\" ) pass @abstractmethod def save_model ( self , model ): log . info ( \"Saving model to ONNX format.\" ) pass @abstractmethod def register_model ( self , model_path ): pass TrainingLoopExample Bases: Train Source code in azure_helper/steps/train.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 class TrainingLoopExample ( Train ): def __init__ ( self , run : Run , trainig_datastore : str , model_name : str , target_name : str , project_name : str , ): \"\"\"Typical example of how you could define a training loop to be used in the `ScriptRunConfig` class in the [submit_run][azure_helper.steps.create_aml_experiment.AMLExperiment] method. you can use it in the following way. ```python if __name__ == \"__main__\": run = Run.get_context() tl = TrainingLoop( run=run, trainig_datastore=\"train_datastore\", model_name=\"model_name\", target_name: \"y\", project_name: \"project\", ) x_train, y_train, x_test, y_test = tl.prepare_data() model = tl.train_model(x_train, y_train) tl.evaluate_model(model, x_test, y_test) model_path = tl.save_model(model) tl.register_model(model_path) ``` Args: run (Run): The run corresponding to your Experiment. trainig_datastore (str): The name of the datastore where you fetch your datasets. model_name (str): The name of the model you train. target_name (str): The name of the target, in your dataset. project_name (str): The name of the project you're working on. \"\"\" self . run = run self . model_name = model_name self . target_name = target_name self . project_name = project_name self . workspace = run . experiment . workspace self . trainig_datastore = trainig_datastore self . datastore = Datastore . get ( run . experiment . workspace , trainig_datastore ) def get_df_from_datastore_path ( self , datastore : Datastore , datastore_path : str , ) -> pd . DataFrame : \"\"\"Utils function to fetch your datas from a datastore. Note that this function is different from [`download_blob_to_df`][azure_helper.utils.blob_storage_interface] method. `BlobStorageInterface.download_blob_to_df` takes datas from a blob in one of your container located in your storage account. We are fetching datas here from a **datastore**, which is a registered blob in yout AZML workspace. Obviously, using `get_df_from_datastore_path` on a Datastore or using `BlobStorageInterface.download_blob_to_df` on the blob corresponding to the Datastore will get you the same result. Args: datastore (Datastore): The name of the registered Datastore in your AZML workspace. datastore_path (str): The path to the datas you're fetching. Returns: The fetched datas as a dataframe. \"\"\" # In our example we only have single files, # but these may be daily data dumps log . info ( f \"Loading dataset { datastore_path } from datastore { datastore . name } \" ) datastore_cfg = [( datastore , datastore_path )] dataset = Dataset . Tabular . from_delimited_files ( path = datastore_cfg , ) return dataset . to_pandas_dataframe () def prepare_data ( self ) -> List [ pd . DataFrame ]: \"\"\"Get all your datas (train, test) at once. Returns: List[pd.DataFrame]: Your datas. \"\"\" x_train = self . get_df_from_datastore_path ( self . datastore , f \" { self . project_name } /train/X_train.csv\" , ) y_train = self . get_df_from_datastore_path ( self . datastore , f \" { self . project_name } /train/y_train.csv\" , ) y_train = y_train [ self . target_name ] x_test = self . get_df_from_datastore_path ( self . datastore , f \" { self . project_name } /test/X_test.csv\" , ) y_test = self . get_df_from_datastore_path ( self . datastore , f \" { self . project_name } /test/y_test.csv\" , ) y_test = y_test [ self . target_name ] return x_train , y_train , x_test , y_test def train_model ( self , x_train : pd . DataFrame , y_train : pd . DataFrame ): \"\"\"Start the training of the model. Args: x_train (pd.DataFrame): Train dataset. y_train (pd.DataFrame): Train target. Returns: _type_: A trained model. \"\"\" log . info ( \"Start training model.\" ) model = LogisticRegression () model . fit ( x_train , y_train ) return model def evaluate_model ( self , model , x_test : pd . DataFrame , y_test : pd . DataFrame ): \"\"\"Evaluate your model and record the corresponding metric. Args: model (_type_): The model you want to evaluate. x_test (pd.DataFrame): Test/Validation dataset. y_test (pd.DataFrame): Test/Validation target. \"\"\" log . info ( \"Start evaluating model.\" ) y_pred = model . predict ( x_test ) model_f1_score = f1_score ( y_test , y_pred ) self . run . log ( \"F1_Score\" , model_f1_score ) def save_model ( self , model ) -> Path : \"\"\"Convert the model to ONNX and save it. Args: model (_type_): Your trained model. Returns: Path: The path where your converted model is located. \"\"\" log . info ( \"Saving model to ONNX format.\" ) output_dir = Path ( \"outputs\" ) output_dir . mkdir ( parents = True , exist_ok = True ) model_path = output_dir / Path ( \"model.onnx\" ) initial_types = [( \"float_input\" , FloatTensorType ([ None , model . n_features_in_ ]))] model_onnx = convert_sklearn ( model , initial_types = initial_types , target_opset = __max_supported_opset__ , ) # Save the model with open ( \"outputs/model.onnx\" , \"wb\" ) as f : f . write ( model_onnx . SerializeToString ()) log . info ( \"Model saved.\" ) return model_path def register_model ( self , model_path : Path ): \"\"\"Register your model into your AZML Model Registry. Args: model_path (Path): The path returned by the function `save_model`. \"\"\" self . run . upload_file ( str ( model_path ), \"outputs/model.onnx\" ) model = self . run . register_model ( model_name = self . model_name , model_path = \"outputs/model.onnx\" , model_framework = Model . Framework . ONNX , ) self . run . log ( \"Model_ID\" , model . id ) log . info ( f \"Model registered with following informations, name : { model . name } , id : { model . id } , version : { model . version } .\" , ) __init__ ( run , trainig_datastore , model_name , target_name , project_name ) Typical example of how you could define a training loop to be used in the ScriptRunConfig class in the submit_run method. you can use it in the following way. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 if __name__ == \"__main__\" : run = Run . get_context () tl = TrainingLoop ( run = run , trainig_datastore = \"train_datastore\" , model_name = \"model_name\" , target_name : \"y\" , project_name : \"project\" , ) x_train , y_train , x_test , y_test = tl . prepare_data () model = tl . train_model ( x_train , y_train ) tl . evaluate_model ( model , x_test , y_test ) model_path = tl . save_model ( model ) tl . register_model ( model_path ) Parameters: Name Type Description Default run Run The run corresponding to your Experiment. required trainig_datastore str The name of the datastore where you fetch your datasets. required model_name str The name of the model you train. required target_name str The name of the target, in your dataset. required project_name str The name of the project you're working on. required Source code in azure_helper/steps/train.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def __init__ ( self , run : Run , trainig_datastore : str , model_name : str , target_name : str , project_name : str , ): \"\"\"Typical example of how you could define a training loop to be used in the `ScriptRunConfig` class in the [submit_run][azure_helper.steps.create_aml_experiment.AMLExperiment] method. you can use it in the following way. ```python if __name__ == \"__main__\": run = Run.get_context() tl = TrainingLoop( run=run, trainig_datastore=\"train_datastore\", model_name=\"model_name\", target_name: \"y\", project_name: \"project\", ) x_train, y_train, x_test, y_test = tl.prepare_data() model = tl.train_model(x_train, y_train) tl.evaluate_model(model, x_test, y_test) model_path = tl.save_model(model) tl.register_model(model_path) ``` Args: run (Run): The run corresponding to your Experiment. trainig_datastore (str): The name of the datastore where you fetch your datasets. model_name (str): The name of the model you train. target_name (str): The name of the target, in your dataset. project_name (str): The name of the project you're working on. \"\"\" self . run = run self . model_name = model_name self . target_name = target_name self . project_name = project_name self . workspace = run . experiment . workspace self . trainig_datastore = trainig_datastore self . datastore = Datastore . get ( run . experiment . workspace , trainig_datastore ) get_df_from_datastore_path ( datastore , datastore_path ) Utils function to fetch your datas from a datastore. Note that this function is different from download_blob_to_df method. BlobStorageInterface.download_blob_to_df takes datas from a blob in one of your container located in your storage account. We are fetching datas here from a datastore , which is a registered blob in yout AZML workspace. Obviously, using get_df_from_datastore_path on a Datastore or using BlobStorageInterface.download_blob_to_df on the blob corresponding to the Datastore will get you the same result. Parameters: Name Type Description Default datastore Datastore The name of the registered Datastore in your AZML workspace. required datastore_path str The path to the datas you're fetching. required Returns: Type Description pd . DataFrame The fetched datas as a dataframe. Source code in azure_helper/steps/train.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def get_df_from_datastore_path ( self , datastore : Datastore , datastore_path : str , ) -> pd . DataFrame : \"\"\"Utils function to fetch your datas from a datastore. Note that this function is different from [`download_blob_to_df`][azure_helper.utils.blob_storage_interface] method. `BlobStorageInterface.download_blob_to_df` takes datas from a blob in one of your container located in your storage account. We are fetching datas here from a **datastore**, which is a registered blob in yout AZML workspace. Obviously, using `get_df_from_datastore_path` on a Datastore or using `BlobStorageInterface.download_blob_to_df` on the blob corresponding to the Datastore will get you the same result. Args: datastore (Datastore): The name of the registered Datastore in your AZML workspace. datastore_path (str): The path to the datas you're fetching. Returns: The fetched datas as a dataframe. \"\"\" # In our example we only have single files, # but these may be daily data dumps log . info ( f \"Loading dataset { datastore_path } from datastore { datastore . name } \" ) datastore_cfg = [( datastore , datastore_path )] dataset = Dataset . Tabular . from_delimited_files ( path = datastore_cfg , ) return dataset . to_pandas_dataframe () prepare_data () Get all your datas (train, test) at once. Returns: Type Description List [ pd . DataFrame ] List[pd.DataFrame]: Your datas. Source code in azure_helper/steps/train.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 def prepare_data ( self ) -> List [ pd . DataFrame ]: \"\"\"Get all your datas (train, test) at once. Returns: List[pd.DataFrame]: Your datas. \"\"\" x_train = self . get_df_from_datastore_path ( self . datastore , f \" { self . project_name } /train/X_train.csv\" , ) y_train = self . get_df_from_datastore_path ( self . datastore , f \" { self . project_name } /train/y_train.csv\" , ) y_train = y_train [ self . target_name ] x_test = self . get_df_from_datastore_path ( self . datastore , f \" { self . project_name } /test/X_test.csv\" , ) y_test = self . get_df_from_datastore_path ( self . datastore , f \" { self . project_name } /test/y_test.csv\" , ) y_test = y_test [ self . target_name ] return x_train , y_train , x_test , y_test train_model ( x_train , y_train ) Start the training of the model. Parameters: Name Type Description Default x_train pd . DataFrame Train dataset. required y_train pd . DataFrame Train target. required Returns: Name Type Description _type_ A trained model. Source code in azure_helper/steps/train.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 def train_model ( self , x_train : pd . DataFrame , y_train : pd . DataFrame ): \"\"\"Start the training of the model. Args: x_train (pd.DataFrame): Train dataset. y_train (pd.DataFrame): Train target. Returns: _type_: A trained model. \"\"\" log . info ( \"Start training model.\" ) model = LogisticRegression () model . fit ( x_train , y_train ) return model evaluate_model ( model , x_test , y_test ) Evaluate your model and record the corresponding metric. Parameters: Name Type Description Default model _type_ The model you want to evaluate. required x_test pd . DataFrame Test/Validation dataset. required y_test pd . DataFrame Test/Validation target. required Source code in azure_helper/steps/train.py 183 184 185 186 187 188 189 190 191 192 193 194 def evaluate_model ( self , model , x_test : pd . DataFrame , y_test : pd . DataFrame ): \"\"\"Evaluate your model and record the corresponding metric. Args: model (_type_): The model you want to evaluate. x_test (pd.DataFrame): Test/Validation dataset. y_test (pd.DataFrame): Test/Validation target. \"\"\" log . info ( \"Start evaluating model.\" ) y_pred = model . predict ( x_test ) model_f1_score = f1_score ( y_test , y_pred ) self . run . log ( \"F1_Score\" , model_f1_score ) save_model ( model ) Convert the model to ONNX and save it. Parameters: Name Type Description Default model _type_ Your trained model. required Returns: Name Type Description Path Path The path where your converted model is located. Source code in azure_helper/steps/train.py 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 def save_model ( self , model ) -> Path : \"\"\"Convert the model to ONNX and save it. Args: model (_type_): Your trained model. Returns: Path: The path where your converted model is located. \"\"\" log . info ( \"Saving model to ONNX format.\" ) output_dir = Path ( \"outputs\" ) output_dir . mkdir ( parents = True , exist_ok = True ) model_path = output_dir / Path ( \"model.onnx\" ) initial_types = [( \"float_input\" , FloatTensorType ([ None , model . n_features_in_ ]))] model_onnx = convert_sklearn ( model , initial_types = initial_types , target_opset = __max_supported_opset__ , ) # Save the model with open ( \"outputs/model.onnx\" , \"wb\" ) as f : f . write ( model_onnx . SerializeToString ()) log . info ( \"Model saved.\" ) return model_path register_model ( model_path ) Register your model into your AZML Model Registry. Parameters: Name Type Description Default model_path Path The path returned by the function save_model . required Source code in azure_helper/steps/train.py 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 def register_model ( self , model_path : Path ): \"\"\"Register your model into your AZML Model Registry. Args: model_path (Path): The path returned by the function `save_model`. \"\"\" self . run . upload_file ( str ( model_path ), \"outputs/model.onnx\" ) model = self . run . register_model ( model_name = self . model_name , model_path = \"outputs/model.onnx\" , model_framework = Model . Framework . ONNX , ) self . run . log ( \"Model_ID\" , model . id ) log . info ( f \"Model registered with following informations, name : { model . name } , id : { model . id } , version : { model . version } .\" , )","title":"Training loop"},{"location":"steps/train/#training-loop-example","text":"","title":"Training Loop Example"},{"location":"steps/train/#azure_helper.steps.train.Train","text":"Bases: ABC Abstract class defining what should be the major steps of a training loop. Parameters: Name Type Description Default ABC class Abstract Class required Source code in azure_helper/steps/train.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 class Train ( ABC ): \"\"\"Abstract class defining what should be the major steps of a training loop. Args: ABC (class): Abstract Class \"\"\" @abstractmethod def get_df_from_datastore_path ( self , datastore , datastore_path ): log . info ( f \"Loading dataset { datastore_path } from datastore { datastore . name } \" ) pass @abstractmethod def prepare_data ( self ): pass @abstractmethod def train_model ( self ): log . info ( \"Start training model.\" ) pass @abstractmethod def evaluate_model ( self ): log . info ( \"Start evaluating model.\" ) pass @abstractmethod def save_model ( self , model ): log . info ( \"Saving model to ONNX format.\" ) pass @abstractmethod def register_model ( self , model_path ): pass","title":"Train"},{"location":"steps/train/#azure_helper.steps.train.TrainingLoopExample","text":"Bases: Train Source code in azure_helper/steps/train.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 class TrainingLoopExample ( Train ): def __init__ ( self , run : Run , trainig_datastore : str , model_name : str , target_name : str , project_name : str , ): \"\"\"Typical example of how you could define a training loop to be used in the `ScriptRunConfig` class in the [submit_run][azure_helper.steps.create_aml_experiment.AMLExperiment] method. you can use it in the following way. ```python if __name__ == \"__main__\": run = Run.get_context() tl = TrainingLoop( run=run, trainig_datastore=\"train_datastore\", model_name=\"model_name\", target_name: \"y\", project_name: \"project\", ) x_train, y_train, x_test, y_test = tl.prepare_data() model = tl.train_model(x_train, y_train) tl.evaluate_model(model, x_test, y_test) model_path = tl.save_model(model) tl.register_model(model_path) ``` Args: run (Run): The run corresponding to your Experiment. trainig_datastore (str): The name of the datastore where you fetch your datasets. model_name (str): The name of the model you train. target_name (str): The name of the target, in your dataset. project_name (str): The name of the project you're working on. \"\"\" self . run = run self . model_name = model_name self . target_name = target_name self . project_name = project_name self . workspace = run . experiment . workspace self . trainig_datastore = trainig_datastore self . datastore = Datastore . get ( run . experiment . workspace , trainig_datastore ) def get_df_from_datastore_path ( self , datastore : Datastore , datastore_path : str , ) -> pd . DataFrame : \"\"\"Utils function to fetch your datas from a datastore. Note that this function is different from [`download_blob_to_df`][azure_helper.utils.blob_storage_interface] method. `BlobStorageInterface.download_blob_to_df` takes datas from a blob in one of your container located in your storage account. We are fetching datas here from a **datastore**, which is a registered blob in yout AZML workspace. Obviously, using `get_df_from_datastore_path` on a Datastore or using `BlobStorageInterface.download_blob_to_df` on the blob corresponding to the Datastore will get you the same result. Args: datastore (Datastore): The name of the registered Datastore in your AZML workspace. datastore_path (str): The path to the datas you're fetching. Returns: The fetched datas as a dataframe. \"\"\" # In our example we only have single files, # but these may be daily data dumps log . info ( f \"Loading dataset { datastore_path } from datastore { datastore . name } \" ) datastore_cfg = [( datastore , datastore_path )] dataset = Dataset . Tabular . from_delimited_files ( path = datastore_cfg , ) return dataset . to_pandas_dataframe () def prepare_data ( self ) -> List [ pd . DataFrame ]: \"\"\"Get all your datas (train, test) at once. Returns: List[pd.DataFrame]: Your datas. \"\"\" x_train = self . get_df_from_datastore_path ( self . datastore , f \" { self . project_name } /train/X_train.csv\" , ) y_train = self . get_df_from_datastore_path ( self . datastore , f \" { self . project_name } /train/y_train.csv\" , ) y_train = y_train [ self . target_name ] x_test = self . get_df_from_datastore_path ( self . datastore , f \" { self . project_name } /test/X_test.csv\" , ) y_test = self . get_df_from_datastore_path ( self . datastore , f \" { self . project_name } /test/y_test.csv\" , ) y_test = y_test [ self . target_name ] return x_train , y_train , x_test , y_test def train_model ( self , x_train : pd . DataFrame , y_train : pd . DataFrame ): \"\"\"Start the training of the model. Args: x_train (pd.DataFrame): Train dataset. y_train (pd.DataFrame): Train target. Returns: _type_: A trained model. \"\"\" log . info ( \"Start training model.\" ) model = LogisticRegression () model . fit ( x_train , y_train ) return model def evaluate_model ( self , model , x_test : pd . DataFrame , y_test : pd . DataFrame ): \"\"\"Evaluate your model and record the corresponding metric. Args: model (_type_): The model you want to evaluate. x_test (pd.DataFrame): Test/Validation dataset. y_test (pd.DataFrame): Test/Validation target. \"\"\" log . info ( \"Start evaluating model.\" ) y_pred = model . predict ( x_test ) model_f1_score = f1_score ( y_test , y_pred ) self . run . log ( \"F1_Score\" , model_f1_score ) def save_model ( self , model ) -> Path : \"\"\"Convert the model to ONNX and save it. Args: model (_type_): Your trained model. Returns: Path: The path where your converted model is located. \"\"\" log . info ( \"Saving model to ONNX format.\" ) output_dir = Path ( \"outputs\" ) output_dir . mkdir ( parents = True , exist_ok = True ) model_path = output_dir / Path ( \"model.onnx\" ) initial_types = [( \"float_input\" , FloatTensorType ([ None , model . n_features_in_ ]))] model_onnx = convert_sklearn ( model , initial_types = initial_types , target_opset = __max_supported_opset__ , ) # Save the model with open ( \"outputs/model.onnx\" , \"wb\" ) as f : f . write ( model_onnx . SerializeToString ()) log . info ( \"Model saved.\" ) return model_path def register_model ( self , model_path : Path ): \"\"\"Register your model into your AZML Model Registry. Args: model_path (Path): The path returned by the function `save_model`. \"\"\" self . run . upload_file ( str ( model_path ), \"outputs/model.onnx\" ) model = self . run . register_model ( model_name = self . model_name , model_path = \"outputs/model.onnx\" , model_framework = Model . Framework . ONNX , ) self . run . log ( \"Model_ID\" , model . id ) log . info ( f \"Model registered with following informations, name : { model . name } , id : { model . id } , version : { model . version } .\" , )","title":"TrainingLoopExample"},{"location":"steps/train/#azure_helper.steps.train.TrainingLoopExample.__init__","text":"Typical example of how you could define a training loop to be used in the ScriptRunConfig class in the submit_run method. you can use it in the following way. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 if __name__ == \"__main__\" : run = Run . get_context () tl = TrainingLoop ( run = run , trainig_datastore = \"train_datastore\" , model_name = \"model_name\" , target_name : \"y\" , project_name : \"project\" , ) x_train , y_train , x_test , y_test = tl . prepare_data () model = tl . train_model ( x_train , y_train ) tl . evaluate_model ( model , x_test , y_test ) model_path = tl . save_model ( model ) tl . register_model ( model_path ) Parameters: Name Type Description Default run Run The run corresponding to your Experiment. required trainig_datastore str The name of the datastore where you fetch your datasets. required model_name str The name of the model you train. required target_name str The name of the target, in your dataset. required project_name str The name of the project you're working on. required Source code in azure_helper/steps/train.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def __init__ ( self , run : Run , trainig_datastore : str , model_name : str , target_name : str , project_name : str , ): \"\"\"Typical example of how you could define a training loop to be used in the `ScriptRunConfig` class in the [submit_run][azure_helper.steps.create_aml_experiment.AMLExperiment] method. you can use it in the following way. ```python if __name__ == \"__main__\": run = Run.get_context() tl = TrainingLoop( run=run, trainig_datastore=\"train_datastore\", model_name=\"model_name\", target_name: \"y\", project_name: \"project\", ) x_train, y_train, x_test, y_test = tl.prepare_data() model = tl.train_model(x_train, y_train) tl.evaluate_model(model, x_test, y_test) model_path = tl.save_model(model) tl.register_model(model_path) ``` Args: run (Run): The run corresponding to your Experiment. trainig_datastore (str): The name of the datastore where you fetch your datasets. model_name (str): The name of the model you train. target_name (str): The name of the target, in your dataset. project_name (str): The name of the project you're working on. \"\"\" self . run = run self . model_name = model_name self . target_name = target_name self . project_name = project_name self . workspace = run . experiment . workspace self . trainig_datastore = trainig_datastore self . datastore = Datastore . get ( run . experiment . workspace , trainig_datastore )","title":"__init__()"},{"location":"steps/train/#azure_helper.steps.train.TrainingLoopExample.get_df_from_datastore_path","text":"Utils function to fetch your datas from a datastore. Note that this function is different from download_blob_to_df method. BlobStorageInterface.download_blob_to_df takes datas from a blob in one of your container located in your storage account. We are fetching datas here from a datastore , which is a registered blob in yout AZML workspace. Obviously, using get_df_from_datastore_path on a Datastore or using BlobStorageInterface.download_blob_to_df on the blob corresponding to the Datastore will get you the same result. Parameters: Name Type Description Default datastore Datastore The name of the registered Datastore in your AZML workspace. required datastore_path str The path to the datas you're fetching. required Returns: Type Description pd . DataFrame The fetched datas as a dataframe. Source code in azure_helper/steps/train.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def get_df_from_datastore_path ( self , datastore : Datastore , datastore_path : str , ) -> pd . DataFrame : \"\"\"Utils function to fetch your datas from a datastore. Note that this function is different from [`download_blob_to_df`][azure_helper.utils.blob_storage_interface] method. `BlobStorageInterface.download_blob_to_df` takes datas from a blob in one of your container located in your storage account. We are fetching datas here from a **datastore**, which is a registered blob in yout AZML workspace. Obviously, using `get_df_from_datastore_path` on a Datastore or using `BlobStorageInterface.download_blob_to_df` on the blob corresponding to the Datastore will get you the same result. Args: datastore (Datastore): The name of the registered Datastore in your AZML workspace. datastore_path (str): The path to the datas you're fetching. Returns: The fetched datas as a dataframe. \"\"\" # In our example we only have single files, # but these may be daily data dumps log . info ( f \"Loading dataset { datastore_path } from datastore { datastore . name } \" ) datastore_cfg = [( datastore , datastore_path )] dataset = Dataset . Tabular . from_delimited_files ( path = datastore_cfg , ) return dataset . to_pandas_dataframe ()","title":"get_df_from_datastore_path()"},{"location":"steps/train/#azure_helper.steps.train.TrainingLoopExample.prepare_data","text":"Get all your datas (train, test) at once. Returns: Type Description List [ pd . DataFrame ] List[pd.DataFrame]: Your datas. Source code in azure_helper/steps/train.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 def prepare_data ( self ) -> List [ pd . DataFrame ]: \"\"\"Get all your datas (train, test) at once. Returns: List[pd.DataFrame]: Your datas. \"\"\" x_train = self . get_df_from_datastore_path ( self . datastore , f \" { self . project_name } /train/X_train.csv\" , ) y_train = self . get_df_from_datastore_path ( self . datastore , f \" { self . project_name } /train/y_train.csv\" , ) y_train = y_train [ self . target_name ] x_test = self . get_df_from_datastore_path ( self . datastore , f \" { self . project_name } /test/X_test.csv\" , ) y_test = self . get_df_from_datastore_path ( self . datastore , f \" { self . project_name } /test/y_test.csv\" , ) y_test = y_test [ self . target_name ] return x_train , y_train , x_test , y_test","title":"prepare_data()"},{"location":"steps/train/#azure_helper.steps.train.TrainingLoopExample.train_model","text":"Start the training of the model. Parameters: Name Type Description Default x_train pd . DataFrame Train dataset. required y_train pd . DataFrame Train target. required Returns: Name Type Description _type_ A trained model. Source code in azure_helper/steps/train.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 def train_model ( self , x_train : pd . DataFrame , y_train : pd . DataFrame ): \"\"\"Start the training of the model. Args: x_train (pd.DataFrame): Train dataset. y_train (pd.DataFrame): Train target. Returns: _type_: A trained model. \"\"\" log . info ( \"Start training model.\" ) model = LogisticRegression () model . fit ( x_train , y_train ) return model","title":"train_model()"},{"location":"steps/train/#azure_helper.steps.train.TrainingLoopExample.evaluate_model","text":"Evaluate your model and record the corresponding metric. Parameters: Name Type Description Default model _type_ The model you want to evaluate. required x_test pd . DataFrame Test/Validation dataset. required y_test pd . DataFrame Test/Validation target. required Source code in azure_helper/steps/train.py 183 184 185 186 187 188 189 190 191 192 193 194 def evaluate_model ( self , model , x_test : pd . DataFrame , y_test : pd . DataFrame ): \"\"\"Evaluate your model and record the corresponding metric. Args: model (_type_): The model you want to evaluate. x_test (pd.DataFrame): Test/Validation dataset. y_test (pd.DataFrame): Test/Validation target. \"\"\" log . info ( \"Start evaluating model.\" ) y_pred = model . predict ( x_test ) model_f1_score = f1_score ( y_test , y_pred ) self . run . log ( \"F1_Score\" , model_f1_score )","title":"evaluate_model()"},{"location":"steps/train/#azure_helper.steps.train.TrainingLoopExample.save_model","text":"Convert the model to ONNX and save it. Parameters: Name Type Description Default model _type_ Your trained model. required Returns: Name Type Description Path Path The path where your converted model is located. Source code in azure_helper/steps/train.py 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 def save_model ( self , model ) -> Path : \"\"\"Convert the model to ONNX and save it. Args: model (_type_): Your trained model. Returns: Path: The path where your converted model is located. \"\"\" log . info ( \"Saving model to ONNX format.\" ) output_dir = Path ( \"outputs\" ) output_dir . mkdir ( parents = True , exist_ok = True ) model_path = output_dir / Path ( \"model.onnx\" ) initial_types = [( \"float_input\" , FloatTensorType ([ None , model . n_features_in_ ]))] model_onnx = convert_sklearn ( model , initial_types = initial_types , target_opset = __max_supported_opset__ , ) # Save the model with open ( \"outputs/model.onnx\" , \"wb\" ) as f : f . write ( model_onnx . SerializeToString ()) log . info ( \"Model saved.\" ) return model_path","title":"save_model()"},{"location":"steps/train/#azure_helper.steps.train.TrainingLoopExample.register_model","text":"Register your model into your AZML Model Registry. Parameters: Name Type Description Default model_path Path The path returned by the function save_model . required Source code in azure_helper/steps/train.py 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 def register_model ( self , model_path : Path ): \"\"\"Register your model into your AZML Model Registry. Args: model_path (Path): The path returned by the function `save_model`. \"\"\" self . run . upload_file ( str ( model_path ), \"outputs/model.onnx\" ) model = self . run . register_model ( model_name = self . model_name , model_path = \"outputs/model.onnx\" , model_framework = Model . Framework . ONNX , ) self . run . log ( \"Model_ID\" , model . id ) log . info ( f \"Model registered with following informations, name : { model . name } , id : { model . id } , version : { model . version } .\" , )","title":"register_model()"},{"location":"utils/aml_interface/","text":"The Azure Machine Learning Interface class azure_helper.utils.aml_interface AMLInterface Source code in azure_helper/utils/aml_interface.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 class AMLInterface : def __init__ ( self , spn_credentials : Dict [ str , str ], subscription_id : str , workspace_name : str , resource_group : str , ): \"\"\"Instantiate the connection to an Azure Machine Learning Workspace. This class is the principal interface with the other ones. It uses a Service Principle **with Password Authentication** connection to interact with Azure Machine Learning. The Service Principle credentials needed can be encapsulated in the following Pydantic class. ```python from pydantic import BaseSettings, Field class SpnCredentials(BaseSettings): tenant_id: str = Field(env=\"TENANT_ID\") service_principal_id: str = Field(env=\"SPN_ID\") service_principal_password: str = Field(env=\"SPN_PASSWORD\") ``` !!! info \"Information\" To create a Service Principle with Password Authentication, you can do it using the azure-cli with the following shell command. `az ad sp create-for-rbac --name <spn-name>` It is important to note down the app id and password from the creation of this service principal! You\u2019ll also need the tenant ID listed here. !!! attention \"Attention\" The service principal also needs to have a contributor role in the IAM management of the ressource group. It is responsible for : * Connect to an existing Storage Account and promote blob inside a specified container into an Azure Machine Learning Datastores. * Register Azure Machine Learning Environment created by the [`AMLEnvironment`][azure_helper.steps.create_aml_env] class as Docker Image. * Provisioning Compute Instance, either by fetching an existing one or creating a new one. Args: spn_credentials (Dict[str, str]): Credentials of the Service Principal used to communicate between the different resources of the workspace. Must contains the TenantID and the ServicePrincipalID. subscription_id (str): The Azure subscription ID containing the workspace. workspace_name (str): The workspace name. The name must be between 2 and 32 characters long. The first character of the name must be alphanumeric (letter or number), but the rest of the name may contain alphanumerics, hyphens, and underscores. Whitespace is not allowed. resource_group (str): The resource group containing the workspace. \"\"\" auth = ServicePrincipalAuthentication ( ** spn_credentials ) self . workspace = Workspace ( workspace_name = workspace_name , auth = auth , subscription_id = subscription_id , resource_group = resource_group , ) def register_datastore ( self , datastore_name : str , container_name : str , storage_acct_name : str , storage_acct_key : str , ): \"\"\"Register an Azure Blob Container as an AZML datastore. ie if you have an architecture like the following one. ```bash Storage_Account : workspaceperso5448820782 \u2502 \u251c\u2500\u2500 Container : project-mlops-mk-5448820782 \u2502 \u251c\u2500\u2500 blob : train \u2502 \u2514\u2500\u2500 blob : test ``` Then ```py aml_interface.register_datastore( datastore_name=\"project_name\", container_name=\"project-mlops-mk-5448820782\", storage_acct_name=\"workspaceperso5448820782\", storage_acct_key=\"storage_acct_key\", ) ``` Will register the Blob container `project-mlops-mk-5448820782` as an AZML datastore under the name `project_name`. !!! attention \"Attention\" We are talking here about **datastores**, not **datasets**, which are special data assets of a datastore. Eg, in the following architecture. ```bash Storage_Account : workspaceperso5448820782 \u2502 \u251c\u2500\u2500 Container : project-mlops-mk-5448820782 \u2502 \u251c\u2500\u2500 blob : train \u2502 \u2502 \u251c\u2500\u2500 x_train.csv \u2502 \u2502 \u2514\u2500\u2500 y_train.csv \u2502 \u2514\u2500\u2500 blob : test ``` `x_train.csv` and `y_train.csv` can be registered as datasets. Args: datastore_name (str): The name of the AZML datastore, case insensitive, can only contain alphanumeric characters and -. container_name (str): The name of the azure blob container. storage_acct_name (str): The storage account name. storage_acct_key (str): Access keys of your storage account, defaults to None. \"\"\" Datastore . register_azure_blob_container ( workspace = self . workspace , datastore_name = datastore_name , container_name = container_name , account_name = storage_acct_name , account_key = storage_acct_key , ) def register_aml_environment ( self , environment : Environment , build_locally : bool = False , ): \"\"\"Register the environment object in your workspace. An environement created by the [`AMLEnvironment`][azure_helper.steps.create_aml_env] class encapsulate in a Docker image everything which is needed to make to project work. See its documentation for further explainations. Args: environment (Environment): A reproducible Python environment for machine learning experiments. build_locally (bool, optional): Whether you want to build locally your environment as a Docker image and push the image to workspace ACR directly. This is recommended when users are iterating on the dockerfile since local build can utilize cached layers. Defaults to False. \"\"\" environment . register ( workspace = self . workspace ) if build_locally : environment . build_local ( self . workspace , useDocker = True , pushImageToWorkspaceAcr = True , ) def get_compute_target ( self , compute_name : str , vm_size : str = \"\" , min_node : int = 1 , max_node : int = 2 , ) -> ComputeTarget : \"\"\"Instantiate a compute instance to train the models. If no Compute Instance with the specified parameters is found in the workspace, a new one will be created. Args: compute_name (str): The name of the compute instance. vm_size (str): The size of agent VMs in the the compute instance. More details can be found [here](https://aka.ms/azureml-vm-details). Note that not all sizes are available in all regions, as detailed in the previous link. If not specified, (ie `vm_size = \"\"`) defaults to `Standard_NC6`. min_node (int, optional): The minimum number of nodes to use on the cluster. Defaults to 1. max_node (int, optional): The maximum number of nodes to use on the cluster. Defaults to 2. Returns: ComputeTarget: An instantiated compute instance. \"\"\" try : compute_target = ComputeTarget ( workspace = self . workspace , name = compute_name , ) log . info ( \"Found existing compute target\" ) log . info ( f \"Compute target instantiated : { compute_target . status . serialize () } \" , ) except ComputeTargetException as err : log . warning ( f \"No compute target found. Creating a new compute target. { err } \" , ) compute_config = AmlCompute . provisioning_configuration ( vm_size = vm_size , min_nodes = min_node , max_nodes = max_node , ) compute_target = ComputeTarget . create ( self . workspace , compute_name , compute_config , ) compute_target . wait_for_completion ( show_output = True , timeout_in_minutes = 10 , ) log . info ( f \"Compute target instantiated : { compute_target . status . serialize () } \" , ) return compute_target __init__ ( spn_credentials , subscription_id , workspace_name , resource_group ) Instantiate the connection to an Azure Machine Learning Workspace. This class is the principal interface with the other ones. It uses a Service Principle with Password Authentication connection to interact with Azure Machine Learning. The Service Principle credentials needed can be encapsulated in the following Pydantic class. 1 2 3 4 5 6 from pydantic import BaseSettings , Field class SpnCredentials ( BaseSettings ): tenant_id : str = Field ( env = \"TENANT_ID\" ) service_principal_id : str = Field ( env = \"SPN_ID\" ) service_principal_password : str = Field ( env = \"SPN_PASSWORD\" ) Information To create a Service Principle with Password Authentication, you can do it using the azure-cli with the following shell command. az ad sp create-for-rbac --name <spn-name> It is important to note down the app id and password from the creation of this service principal! You\u2019ll also need the tenant ID listed here. Attention The service principal also needs to have a contributor role in the IAM management of the ressource group. It is responsible for : Connect to an existing Storage Account and promote blob inside a specified container into an Azure Machine Learning Datastores. Register Azure Machine Learning Environment created by the AMLEnvironment class as Docker Image. Provisioning Compute Instance, either by fetching an existing one or creating a new one. Parameters: Name Type Description Default spn_credentials Dict [ str , str ] Credentials of the Service Principal used to communicate between the different resources of the workspace. Must contains the TenantID and the ServicePrincipalID. required subscription_id str The Azure subscription ID containing the workspace. required workspace_name str The workspace name. The name must be between 2 and 32 characters long. The first character of the name must be alphanumeric (letter or number), but the rest of the name may contain alphanumerics, hyphens, and underscores. Whitespace is not allowed. required resource_group str The resource group containing the workspace. required Source code in azure_helper/utils/aml_interface.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def __init__ ( self , spn_credentials : Dict [ str , str ], subscription_id : str , workspace_name : str , resource_group : str , ): \"\"\"Instantiate the connection to an Azure Machine Learning Workspace. This class is the principal interface with the other ones. It uses a Service Principle **with Password Authentication** connection to interact with Azure Machine Learning. The Service Principle credentials needed can be encapsulated in the following Pydantic class. ```python from pydantic import BaseSettings, Field class SpnCredentials(BaseSettings): tenant_id: str = Field(env=\"TENANT_ID\") service_principal_id: str = Field(env=\"SPN_ID\") service_principal_password: str = Field(env=\"SPN_PASSWORD\") ``` !!! info \"Information\" To create a Service Principle with Password Authentication, you can do it using the azure-cli with the following shell command. `az ad sp create-for-rbac --name <spn-name>` It is important to note down the app id and password from the creation of this service principal! You\u2019ll also need the tenant ID listed here. !!! attention \"Attention\" The service principal also needs to have a contributor role in the IAM management of the ressource group. It is responsible for : * Connect to an existing Storage Account and promote blob inside a specified container into an Azure Machine Learning Datastores. * Register Azure Machine Learning Environment created by the [`AMLEnvironment`][azure_helper.steps.create_aml_env] class as Docker Image. * Provisioning Compute Instance, either by fetching an existing one or creating a new one. Args: spn_credentials (Dict[str, str]): Credentials of the Service Principal used to communicate between the different resources of the workspace. Must contains the TenantID and the ServicePrincipalID. subscription_id (str): The Azure subscription ID containing the workspace. workspace_name (str): The workspace name. The name must be between 2 and 32 characters long. The first character of the name must be alphanumeric (letter or number), but the rest of the name may contain alphanumerics, hyphens, and underscores. Whitespace is not allowed. resource_group (str): The resource group containing the workspace. \"\"\" auth = ServicePrincipalAuthentication ( ** spn_credentials ) self . workspace = Workspace ( workspace_name = workspace_name , auth = auth , subscription_id = subscription_id , resource_group = resource_group , ) register_datastore ( datastore_name , container_name , storage_acct_name , storage_acct_key ) Register an Azure Blob Container as an AZML datastore. ie if you have an architecture like the following one. 1 2 3 4 5 Storage_Account : workspaceperso5448820782 \u2502 \u251c\u2500\u2500 Container : project-mlops-mk-5448820782 \u2502 \u251c\u2500\u2500 blob : train \u2502 \u2514\u2500\u2500 blob : test Then 1 2 3 4 5 6 aml_interface . register_datastore ( datastore_name = \"project_name\" , container_name = \"project-mlops-mk-5448820782\" , storage_acct_name = \"workspaceperso5448820782\" , storage_acct_key = \"storage_acct_key\" , ) Will register the Blob container project-mlops-mk-5448820782 as an AZML datastore under the name project_name . Attention We are talking here about datastores , not datasets , which are special data assets of a datastore. Eg, in the following architecture. 1 2 3 4 5 6 7 Storage_Account : workspaceperso5448820782 \u2502 \u251c\u2500\u2500 Container : project-mlops-mk-5448820782 \u2502 \u251c\u2500\u2500 blob : train \u2502 \u2502 \u251c\u2500\u2500 x_train.csv \u2502 \u2502 \u2514\u2500\u2500 y_train.csv \u2502 \u2514\u2500\u2500 blob : test x_train.csv and y_train.csv can be registered as datasets. Parameters: Name Type Description Default datastore_name str The name of the AZML datastore, case insensitive, can only contain alphanumeric characters and -. required container_name str The name of the azure blob container. required storage_acct_name str The storage account name. required storage_acct_key str Access keys of your storage account, defaults to None. required Source code in azure_helper/utils/aml_interface.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 def register_datastore ( self , datastore_name : str , container_name : str , storage_acct_name : str , storage_acct_key : str , ): \"\"\"Register an Azure Blob Container as an AZML datastore. ie if you have an architecture like the following one. ```bash Storage_Account : workspaceperso5448820782 \u2502 \u251c\u2500\u2500 Container : project-mlops-mk-5448820782 \u2502 \u251c\u2500\u2500 blob : train \u2502 \u2514\u2500\u2500 blob : test ``` Then ```py aml_interface.register_datastore( datastore_name=\"project_name\", container_name=\"project-mlops-mk-5448820782\", storage_acct_name=\"workspaceperso5448820782\", storage_acct_key=\"storage_acct_key\", ) ``` Will register the Blob container `project-mlops-mk-5448820782` as an AZML datastore under the name `project_name`. !!! attention \"Attention\" We are talking here about **datastores**, not **datasets**, which are special data assets of a datastore. Eg, in the following architecture. ```bash Storage_Account : workspaceperso5448820782 \u2502 \u251c\u2500\u2500 Container : project-mlops-mk-5448820782 \u2502 \u251c\u2500\u2500 blob : train \u2502 \u2502 \u251c\u2500\u2500 x_train.csv \u2502 \u2502 \u2514\u2500\u2500 y_train.csv \u2502 \u2514\u2500\u2500 blob : test ``` `x_train.csv` and `y_train.csv` can be registered as datasets. Args: datastore_name (str): The name of the AZML datastore, case insensitive, can only contain alphanumeric characters and -. container_name (str): The name of the azure blob container. storage_acct_name (str): The storage account name. storage_acct_key (str): Access keys of your storage account, defaults to None. \"\"\" Datastore . register_azure_blob_container ( workspace = self . workspace , datastore_name = datastore_name , container_name = container_name , account_name = storage_acct_name , account_key = storage_acct_key , ) register_aml_environment ( environment , build_locally = False ) Register the environment object in your workspace. An environement created by the AMLEnvironment class encapsulate in a Docker image everything which is needed to make to project work. See its documentation for further explainations. Parameters: Name Type Description Default environment Environment A reproducible Python environment for machine learning experiments. required build_locally bool Whether you want to build locally your environment as a Docker image and push the image to workspace ACR directly. This is recommended when users are iterating on the dockerfile since local build can utilize cached layers. Defaults to False. False Source code in azure_helper/utils/aml_interface.py 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 def register_aml_environment ( self , environment : Environment , build_locally : bool = False , ): \"\"\"Register the environment object in your workspace. An environement created by the [`AMLEnvironment`][azure_helper.steps.create_aml_env] class encapsulate in a Docker image everything which is needed to make to project work. See its documentation for further explainations. Args: environment (Environment): A reproducible Python environment for machine learning experiments. build_locally (bool, optional): Whether you want to build locally your environment as a Docker image and push the image to workspace ACR directly. This is recommended when users are iterating on the dockerfile since local build can utilize cached layers. Defaults to False. \"\"\" environment . register ( workspace = self . workspace ) if build_locally : environment . build_local ( self . workspace , useDocker = True , pushImageToWorkspaceAcr = True , ) get_compute_target ( compute_name , vm_size = '' , min_node = 1 , max_node = 2 ) Instantiate a compute instance to train the models. If no Compute Instance with the specified parameters is found in the workspace, a new one will be created. Parameters: Name Type Description Default compute_name str The name of the compute instance. required vm_size str The size of agent VMs in the the compute instance. More details can be found here . Note that not all sizes are available in all regions, as detailed in the previous link. If not specified, (ie vm_size = \"\" ) defaults to Standard_NC6 . '' min_node int The minimum number of nodes to use on the cluster. Defaults to 1. 1 max_node int The maximum number of nodes to use on the cluster. Defaults to 2. 2 Returns: Name Type Description ComputeTarget ComputeTarget An instantiated compute instance. Source code in azure_helper/utils/aml_interface.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 def get_compute_target ( self , compute_name : str , vm_size : str = \"\" , min_node : int = 1 , max_node : int = 2 , ) -> ComputeTarget : \"\"\"Instantiate a compute instance to train the models. If no Compute Instance with the specified parameters is found in the workspace, a new one will be created. Args: compute_name (str): The name of the compute instance. vm_size (str): The size of agent VMs in the the compute instance. More details can be found [here](https://aka.ms/azureml-vm-details). Note that not all sizes are available in all regions, as detailed in the previous link. If not specified, (ie `vm_size = \"\"`) defaults to `Standard_NC6`. min_node (int, optional): The minimum number of nodes to use on the cluster. Defaults to 1. max_node (int, optional): The maximum number of nodes to use on the cluster. Defaults to 2. Returns: ComputeTarget: An instantiated compute instance. \"\"\" try : compute_target = ComputeTarget ( workspace = self . workspace , name = compute_name , ) log . info ( \"Found existing compute target\" ) log . info ( f \"Compute target instantiated : { compute_target . status . serialize () } \" , ) except ComputeTargetException as err : log . warning ( f \"No compute target found. Creating a new compute target. { err } \" , ) compute_config = AmlCompute . provisioning_configuration ( vm_size = vm_size , min_nodes = min_node , max_nodes = max_node , ) compute_target = ComputeTarget . create ( self . workspace , compute_name , compute_config , ) compute_target . wait_for_completion ( show_output = True , timeout_in_minutes = 10 , ) log . info ( f \"Compute target instantiated : { compute_target . status . serialize () } \" , ) return compute_target","title":"AML Interface"},{"location":"utils/aml_interface/#the-azure-machine-learning-interface-class","text":"","title":"The Azure Machine Learning Interface class"},{"location":"utils/aml_interface/#azure_helper.utils.aml_interface","text":"","title":"aml_interface"},{"location":"utils/aml_interface/#azure_helper.utils.aml_interface.AMLInterface","text":"Source code in azure_helper/utils/aml_interface.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 class AMLInterface : def __init__ ( self , spn_credentials : Dict [ str , str ], subscription_id : str , workspace_name : str , resource_group : str , ): \"\"\"Instantiate the connection to an Azure Machine Learning Workspace. This class is the principal interface with the other ones. It uses a Service Principle **with Password Authentication** connection to interact with Azure Machine Learning. The Service Principle credentials needed can be encapsulated in the following Pydantic class. ```python from pydantic import BaseSettings, Field class SpnCredentials(BaseSettings): tenant_id: str = Field(env=\"TENANT_ID\") service_principal_id: str = Field(env=\"SPN_ID\") service_principal_password: str = Field(env=\"SPN_PASSWORD\") ``` !!! info \"Information\" To create a Service Principle with Password Authentication, you can do it using the azure-cli with the following shell command. `az ad sp create-for-rbac --name <spn-name>` It is important to note down the app id and password from the creation of this service principal! You\u2019ll also need the tenant ID listed here. !!! attention \"Attention\" The service principal also needs to have a contributor role in the IAM management of the ressource group. It is responsible for : * Connect to an existing Storage Account and promote blob inside a specified container into an Azure Machine Learning Datastores. * Register Azure Machine Learning Environment created by the [`AMLEnvironment`][azure_helper.steps.create_aml_env] class as Docker Image. * Provisioning Compute Instance, either by fetching an existing one or creating a new one. Args: spn_credentials (Dict[str, str]): Credentials of the Service Principal used to communicate between the different resources of the workspace. Must contains the TenantID and the ServicePrincipalID. subscription_id (str): The Azure subscription ID containing the workspace. workspace_name (str): The workspace name. The name must be between 2 and 32 characters long. The first character of the name must be alphanumeric (letter or number), but the rest of the name may contain alphanumerics, hyphens, and underscores. Whitespace is not allowed. resource_group (str): The resource group containing the workspace. \"\"\" auth = ServicePrincipalAuthentication ( ** spn_credentials ) self . workspace = Workspace ( workspace_name = workspace_name , auth = auth , subscription_id = subscription_id , resource_group = resource_group , ) def register_datastore ( self , datastore_name : str , container_name : str , storage_acct_name : str , storage_acct_key : str , ): \"\"\"Register an Azure Blob Container as an AZML datastore. ie if you have an architecture like the following one. ```bash Storage_Account : workspaceperso5448820782 \u2502 \u251c\u2500\u2500 Container : project-mlops-mk-5448820782 \u2502 \u251c\u2500\u2500 blob : train \u2502 \u2514\u2500\u2500 blob : test ``` Then ```py aml_interface.register_datastore( datastore_name=\"project_name\", container_name=\"project-mlops-mk-5448820782\", storage_acct_name=\"workspaceperso5448820782\", storage_acct_key=\"storage_acct_key\", ) ``` Will register the Blob container `project-mlops-mk-5448820782` as an AZML datastore under the name `project_name`. !!! attention \"Attention\" We are talking here about **datastores**, not **datasets**, which are special data assets of a datastore. Eg, in the following architecture. ```bash Storage_Account : workspaceperso5448820782 \u2502 \u251c\u2500\u2500 Container : project-mlops-mk-5448820782 \u2502 \u251c\u2500\u2500 blob : train \u2502 \u2502 \u251c\u2500\u2500 x_train.csv \u2502 \u2502 \u2514\u2500\u2500 y_train.csv \u2502 \u2514\u2500\u2500 blob : test ``` `x_train.csv` and `y_train.csv` can be registered as datasets. Args: datastore_name (str): The name of the AZML datastore, case insensitive, can only contain alphanumeric characters and -. container_name (str): The name of the azure blob container. storage_acct_name (str): The storage account name. storage_acct_key (str): Access keys of your storage account, defaults to None. \"\"\" Datastore . register_azure_blob_container ( workspace = self . workspace , datastore_name = datastore_name , container_name = container_name , account_name = storage_acct_name , account_key = storage_acct_key , ) def register_aml_environment ( self , environment : Environment , build_locally : bool = False , ): \"\"\"Register the environment object in your workspace. An environement created by the [`AMLEnvironment`][azure_helper.steps.create_aml_env] class encapsulate in a Docker image everything which is needed to make to project work. See its documentation for further explainations. Args: environment (Environment): A reproducible Python environment for machine learning experiments. build_locally (bool, optional): Whether you want to build locally your environment as a Docker image and push the image to workspace ACR directly. This is recommended when users are iterating on the dockerfile since local build can utilize cached layers. Defaults to False. \"\"\" environment . register ( workspace = self . workspace ) if build_locally : environment . build_local ( self . workspace , useDocker = True , pushImageToWorkspaceAcr = True , ) def get_compute_target ( self , compute_name : str , vm_size : str = \"\" , min_node : int = 1 , max_node : int = 2 , ) -> ComputeTarget : \"\"\"Instantiate a compute instance to train the models. If no Compute Instance with the specified parameters is found in the workspace, a new one will be created. Args: compute_name (str): The name of the compute instance. vm_size (str): The size of agent VMs in the the compute instance. More details can be found [here](https://aka.ms/azureml-vm-details). Note that not all sizes are available in all regions, as detailed in the previous link. If not specified, (ie `vm_size = \"\"`) defaults to `Standard_NC6`. min_node (int, optional): The minimum number of nodes to use on the cluster. Defaults to 1. max_node (int, optional): The maximum number of nodes to use on the cluster. Defaults to 2. Returns: ComputeTarget: An instantiated compute instance. \"\"\" try : compute_target = ComputeTarget ( workspace = self . workspace , name = compute_name , ) log . info ( \"Found existing compute target\" ) log . info ( f \"Compute target instantiated : { compute_target . status . serialize () } \" , ) except ComputeTargetException as err : log . warning ( f \"No compute target found. Creating a new compute target. { err } \" , ) compute_config = AmlCompute . provisioning_configuration ( vm_size = vm_size , min_nodes = min_node , max_nodes = max_node , ) compute_target = ComputeTarget . create ( self . workspace , compute_name , compute_config , ) compute_target . wait_for_completion ( show_output = True , timeout_in_minutes = 10 , ) log . info ( f \"Compute target instantiated : { compute_target . status . serialize () } \" , ) return compute_target","title":"AMLInterface"},{"location":"utils/aml_interface/#azure_helper.utils.aml_interface.AMLInterface.__init__","text":"Instantiate the connection to an Azure Machine Learning Workspace. This class is the principal interface with the other ones. It uses a Service Principle with Password Authentication connection to interact with Azure Machine Learning. The Service Principle credentials needed can be encapsulated in the following Pydantic class. 1 2 3 4 5 6 from pydantic import BaseSettings , Field class SpnCredentials ( BaseSettings ): tenant_id : str = Field ( env = \"TENANT_ID\" ) service_principal_id : str = Field ( env = \"SPN_ID\" ) service_principal_password : str = Field ( env = \"SPN_PASSWORD\" ) Information To create a Service Principle with Password Authentication, you can do it using the azure-cli with the following shell command. az ad sp create-for-rbac --name <spn-name> It is important to note down the app id and password from the creation of this service principal! You\u2019ll also need the tenant ID listed here. Attention The service principal also needs to have a contributor role in the IAM management of the ressource group. It is responsible for : Connect to an existing Storage Account and promote blob inside a specified container into an Azure Machine Learning Datastores. Register Azure Machine Learning Environment created by the AMLEnvironment class as Docker Image. Provisioning Compute Instance, either by fetching an existing one or creating a new one. Parameters: Name Type Description Default spn_credentials Dict [ str , str ] Credentials of the Service Principal used to communicate between the different resources of the workspace. Must contains the TenantID and the ServicePrincipalID. required subscription_id str The Azure subscription ID containing the workspace. required workspace_name str The workspace name. The name must be between 2 and 32 characters long. The first character of the name must be alphanumeric (letter or number), but the rest of the name may contain alphanumerics, hyphens, and underscores. Whitespace is not allowed. required resource_group str The resource group containing the workspace. required Source code in azure_helper/utils/aml_interface.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def __init__ ( self , spn_credentials : Dict [ str , str ], subscription_id : str , workspace_name : str , resource_group : str , ): \"\"\"Instantiate the connection to an Azure Machine Learning Workspace. This class is the principal interface with the other ones. It uses a Service Principle **with Password Authentication** connection to interact with Azure Machine Learning. The Service Principle credentials needed can be encapsulated in the following Pydantic class. ```python from pydantic import BaseSettings, Field class SpnCredentials(BaseSettings): tenant_id: str = Field(env=\"TENANT_ID\") service_principal_id: str = Field(env=\"SPN_ID\") service_principal_password: str = Field(env=\"SPN_PASSWORD\") ``` !!! info \"Information\" To create a Service Principle with Password Authentication, you can do it using the azure-cli with the following shell command. `az ad sp create-for-rbac --name <spn-name>` It is important to note down the app id and password from the creation of this service principal! You\u2019ll also need the tenant ID listed here. !!! attention \"Attention\" The service principal also needs to have a contributor role in the IAM management of the ressource group. It is responsible for : * Connect to an existing Storage Account and promote blob inside a specified container into an Azure Machine Learning Datastores. * Register Azure Machine Learning Environment created by the [`AMLEnvironment`][azure_helper.steps.create_aml_env] class as Docker Image. * Provisioning Compute Instance, either by fetching an existing one or creating a new one. Args: spn_credentials (Dict[str, str]): Credentials of the Service Principal used to communicate between the different resources of the workspace. Must contains the TenantID and the ServicePrincipalID. subscription_id (str): The Azure subscription ID containing the workspace. workspace_name (str): The workspace name. The name must be between 2 and 32 characters long. The first character of the name must be alphanumeric (letter or number), but the rest of the name may contain alphanumerics, hyphens, and underscores. Whitespace is not allowed. resource_group (str): The resource group containing the workspace. \"\"\" auth = ServicePrincipalAuthentication ( ** spn_credentials ) self . workspace = Workspace ( workspace_name = workspace_name , auth = auth , subscription_id = subscription_id , resource_group = resource_group , )","title":"__init__()"},{"location":"utils/aml_interface/#azure_helper.utils.aml_interface.AMLInterface.register_datastore","text":"Register an Azure Blob Container as an AZML datastore. ie if you have an architecture like the following one. 1 2 3 4 5 Storage_Account : workspaceperso5448820782 \u2502 \u251c\u2500\u2500 Container : project-mlops-mk-5448820782 \u2502 \u251c\u2500\u2500 blob : train \u2502 \u2514\u2500\u2500 blob : test Then 1 2 3 4 5 6 aml_interface . register_datastore ( datastore_name = \"project_name\" , container_name = \"project-mlops-mk-5448820782\" , storage_acct_name = \"workspaceperso5448820782\" , storage_acct_key = \"storage_acct_key\" , ) Will register the Blob container project-mlops-mk-5448820782 as an AZML datastore under the name project_name . Attention We are talking here about datastores , not datasets , which are special data assets of a datastore. Eg, in the following architecture. 1 2 3 4 5 6 7 Storage_Account : workspaceperso5448820782 \u2502 \u251c\u2500\u2500 Container : project-mlops-mk-5448820782 \u2502 \u251c\u2500\u2500 blob : train \u2502 \u2502 \u251c\u2500\u2500 x_train.csv \u2502 \u2502 \u2514\u2500\u2500 y_train.csv \u2502 \u2514\u2500\u2500 blob : test x_train.csv and y_train.csv can be registered as datasets. Parameters: Name Type Description Default datastore_name str The name of the AZML datastore, case insensitive, can only contain alphanumeric characters and -. required container_name str The name of the azure blob container. required storage_acct_name str The storage account name. required storage_acct_key str Access keys of your storage account, defaults to None. required Source code in azure_helper/utils/aml_interface.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 def register_datastore ( self , datastore_name : str , container_name : str , storage_acct_name : str , storage_acct_key : str , ): \"\"\"Register an Azure Blob Container as an AZML datastore. ie if you have an architecture like the following one. ```bash Storage_Account : workspaceperso5448820782 \u2502 \u251c\u2500\u2500 Container : project-mlops-mk-5448820782 \u2502 \u251c\u2500\u2500 blob : train \u2502 \u2514\u2500\u2500 blob : test ``` Then ```py aml_interface.register_datastore( datastore_name=\"project_name\", container_name=\"project-mlops-mk-5448820782\", storage_acct_name=\"workspaceperso5448820782\", storage_acct_key=\"storage_acct_key\", ) ``` Will register the Blob container `project-mlops-mk-5448820782` as an AZML datastore under the name `project_name`. !!! attention \"Attention\" We are talking here about **datastores**, not **datasets**, which are special data assets of a datastore. Eg, in the following architecture. ```bash Storage_Account : workspaceperso5448820782 \u2502 \u251c\u2500\u2500 Container : project-mlops-mk-5448820782 \u2502 \u251c\u2500\u2500 blob : train \u2502 \u2502 \u251c\u2500\u2500 x_train.csv \u2502 \u2502 \u2514\u2500\u2500 y_train.csv \u2502 \u2514\u2500\u2500 blob : test ``` `x_train.csv` and `y_train.csv` can be registered as datasets. Args: datastore_name (str): The name of the AZML datastore, case insensitive, can only contain alphanumeric characters and -. container_name (str): The name of the azure blob container. storage_acct_name (str): The storage account name. storage_acct_key (str): Access keys of your storage account, defaults to None. \"\"\" Datastore . register_azure_blob_container ( workspace = self . workspace , datastore_name = datastore_name , container_name = container_name , account_name = storage_acct_name , account_key = storage_acct_key , )","title":"register_datastore()"},{"location":"utils/aml_interface/#azure_helper.utils.aml_interface.AMLInterface.register_aml_environment","text":"Register the environment object in your workspace. An environement created by the AMLEnvironment class encapsulate in a Docker image everything which is needed to make to project work. See its documentation for further explainations. Parameters: Name Type Description Default environment Environment A reproducible Python environment for machine learning experiments. required build_locally bool Whether you want to build locally your environment as a Docker image and push the image to workspace ACR directly. This is recommended when users are iterating on the dockerfile since local build can utilize cached layers. Defaults to False. False Source code in azure_helper/utils/aml_interface.py 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 def register_aml_environment ( self , environment : Environment , build_locally : bool = False , ): \"\"\"Register the environment object in your workspace. An environement created by the [`AMLEnvironment`][azure_helper.steps.create_aml_env] class encapsulate in a Docker image everything which is needed to make to project work. See its documentation for further explainations. Args: environment (Environment): A reproducible Python environment for machine learning experiments. build_locally (bool, optional): Whether you want to build locally your environment as a Docker image and push the image to workspace ACR directly. This is recommended when users are iterating on the dockerfile since local build can utilize cached layers. Defaults to False. \"\"\" environment . register ( workspace = self . workspace ) if build_locally : environment . build_local ( self . workspace , useDocker = True , pushImageToWorkspaceAcr = True , )","title":"register_aml_environment()"},{"location":"utils/aml_interface/#azure_helper.utils.aml_interface.AMLInterface.get_compute_target","text":"Instantiate a compute instance to train the models. If no Compute Instance with the specified parameters is found in the workspace, a new one will be created. Parameters: Name Type Description Default compute_name str The name of the compute instance. required vm_size str The size of agent VMs in the the compute instance. More details can be found here . Note that not all sizes are available in all regions, as detailed in the previous link. If not specified, (ie vm_size = \"\" ) defaults to Standard_NC6 . '' min_node int The minimum number of nodes to use on the cluster. Defaults to 1. 1 max_node int The maximum number of nodes to use on the cluster. Defaults to 2. 2 Returns: Name Type Description ComputeTarget ComputeTarget An instantiated compute instance. Source code in azure_helper/utils/aml_interface.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 def get_compute_target ( self , compute_name : str , vm_size : str = \"\" , min_node : int = 1 , max_node : int = 2 , ) -> ComputeTarget : \"\"\"Instantiate a compute instance to train the models. If no Compute Instance with the specified parameters is found in the workspace, a new one will be created. Args: compute_name (str): The name of the compute instance. vm_size (str): The size of agent VMs in the the compute instance. More details can be found [here](https://aka.ms/azureml-vm-details). Note that not all sizes are available in all regions, as detailed in the previous link. If not specified, (ie `vm_size = \"\"`) defaults to `Standard_NC6`. min_node (int, optional): The minimum number of nodes to use on the cluster. Defaults to 1. max_node (int, optional): The maximum number of nodes to use on the cluster. Defaults to 2. Returns: ComputeTarget: An instantiated compute instance. \"\"\" try : compute_target = ComputeTarget ( workspace = self . workspace , name = compute_name , ) log . info ( \"Found existing compute target\" ) log . info ( f \"Compute target instantiated : { compute_target . status . serialize () } \" , ) except ComputeTargetException as err : log . warning ( f \"No compute target found. Creating a new compute target. { err } \" , ) compute_config = AmlCompute . provisioning_configuration ( vm_size = vm_size , min_nodes = min_node , max_nodes = max_node , ) compute_target = ComputeTarget . create ( self . workspace , compute_name , compute_config , ) compute_target . wait_for_completion ( show_output = True , timeout_in_minutes = 10 , ) log . info ( f \"Compute target instantiated : { compute_target . status . serialize () } \" , ) return compute_target","title":"get_compute_target()"},{"location":"utils/blob_storage_interface/","text":"The Blob Storage Interface class azure_helper.utils.blob_storage_interface BlobStorageInterface Source code in azure_helper/utils/blob_storage_interface.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 class BlobStorageInterface : def __init__ ( self , storage_acct_name : str , storage_acct_key : str ): \"\"\"Class responsible to interact with an existing Azure Storage Account. It uses a connection string to connect to the Storage Account. ```python conn_str = ( \"DefaultEndpointsProtocol=https;\" + f\"AccountName={storage_acct_name};\" + f\"AccountKey={storage_acct_key};\" + \"EndpointSuffix=core.windows.net\" ) ``` !!! info \"Information\" To get the key of this storage account we use the following command with the azure-cli. ```sh az storage account keys list --account-name <storage-account-name> --resource-group <resource-group> ``` This class is responsible for : * Creating a container in the storage account. * Uploading a dataframe (as a `csv` for now) inside a blob in one of the container of the storage account. * Download a `csv` from a blob in one of the container of the storage account and render it as a pandas dataframe. Args: storage_acct_name (str): The name of the storage account to which you want to connect. storage_acct_key (str): The account key of the storage account. \"\"\" conn_str = ( \"DefaultEndpointsProtocol=https;\" + f \"AccountName= { storage_acct_name } ;\" + f \"AccountKey= { storage_acct_key } ;\" + \"EndpointSuffix=core.windows.net\" ) self . blob_service_client = BlobServiceClient . from_connection_string ( conn_str , ) def create_container ( self , container_name : str ): \"\"\"Create a container inside the storage account. Args: container_name (str): the name of the container you want to create. This name can only contains alphanumeric numbers and dashes '-'. \"\"\" try : self . blob_service_client . create_container ( container_name ) log . info ( f \"Creating blob storage container { container_name } .\" ) except ResourceExistsError : log . warning ( f \"Blob storage container { container_name } already exists.\" ) pass def upload_df_to_blob ( self , dataframe : pd . DataFrame , container_name : str , blob_path : str , ): \"\"\"Upload a pandas dataframe as a `csv` file inside a blob. Eg the following code. ```python from azure_helper.utils.blob_storage_interface import BlobStorageInterface blob_storage_interface = BlobStorageInterface( storage_acct_name=\"workspaceperso5448820782\", storage_acct_key=\"XXXXX-XXXX-XXXXX-XXXX\", ) blob_storage_interface.create_container(container_name=\"project-mlops-mk-5448820782\") blob_storage_interface.upload_df_to_blob( dataframe=x_train, container_name=\"project-mlops-mk-5448820782\", blob_path=\"train/x_train.csv\", ) blob_storage_interface.upload_df_to_blob( dataframe=x_train, container_name=\"project-mlops-mk-5448820782\", blob_path=\"train/y_train.csv\", ) ``` Upload the dataframes `x_train` and `y_train` as `x_train.csv` and `y_train.csv` in the following way. ```bash Storage_Account : workspaceperso5448820782 \u2502 \u251c\u2500\u2500 Container : project-mlops-mk-5448820782 \u2502 \u251c\u2500\u2500 blob : train \u2502 \u2502 \u251c\u2500\u2500 x_train.csv \u2502 \u2502 \u2514\u2500\u2500 y_train.csv \u2502 \u2514\u2500\u2500 blob : test ``` !!! attention \"Attention\" As of now, there is no **data versioning**. Meaning that if the `blob_path` already exists, it will be overwritten with new datas. Args: dataframe (pd.DataFrame): The dataframe you want to upload. container_name (str): The name of the container on which you want to upload the dataframe. blob_path (str): The path to the csv \"\"\" self . create_container ( container_name ) blob_client = self . blob_service_client . get_blob_client ( container = container_name , blob = blob_path , ) try : blob_client . upload_blob ( dataframe . to_csv ( index = False , header = True ) . encode (), ) log . info ( f \"Dataset uploaded at blob path : { blob_path } .\" ) except ResourceExistsError : log . warning ( f \"Blob path { blob_path } already contains datas. Now deleting old datas tu upload the new ones.\" , ) blob_client . delete_blob () blob_client . upload_blob ( dataframe . to_csv ( index = False , header = True ) . encode (), ) log . info ( f \"New dataset uploaded at blob path : { blob_path } .\" ) def download_blob_to_df ( self , container_name : str , blob_path : str ) -> pd . DataFrame : \"\"\"Download a `csv` file a the given `blob_path` location and renders it as a pandas datatrame. ```bash Storage_Account : workspaceperso5448820782 \u2502 \u251c\u2500\u2500 Container : project-mlops-mk-5448820782 \u2502 \u251c\u2500\u2500 blob : train \u2502 \u2502 \u251c\u2500\u2500 x_train.csv \u2502 \u2502 \u2514\u2500\u2500 y_train.csv \u2502 \u2514\u2500\u2500 blob : test ``` ```python from azure_helper.utils.blob_storage_interface import BlobStorageInterface blob_storage_interface = BlobStorageInterface( storage_acct_name=\"workspaceperso5448820782\", storage_acct_key=\"XXXXX-XXXX-XXXXX-XXXX\", ) df = blob_storage_interface.download_blob_to_df( container_name=\"project-mlops-mk-5448820782\", blob_path=\"train/x_train.csv\", ) ``` Args: container_name (str): The name of the container. blob_path (str): The path to the `csv` file. Returns: pd.DataFrame: the `csv` file as a dataframe. \"\"\" blob_client = self . blob_service_client . get_blob_client ( container = container_name , blob = blob_path , ) stream = blob_client . download_blob () buffer = StringIO ( stream . content_as_text ()) dataframe = pd . read_csv ( buffer ) log . info ( f \"Download from { container_name } ended successfully.\" ) return dataframe __init__ ( storage_acct_name , storage_acct_key ) Class responsible to interact with an existing Azure Storage Account. It uses a connection string to connect to the Storage Account. 1 2 3 4 5 6 conn_str = ( \"DefaultEndpointsProtocol=https;\" + f \"AccountName= { storage_acct_name } ;\" + f \"AccountKey= { storage_acct_key } ;\" + \"EndpointSuffix=core.windows.net\" ) Information To get the key of this storage account we use the following command with the azure-cli. 1 az storage account keys list --account-name <storage-account-name> --resource-group <resource-group> This class is responsible for : Creating a container in the storage account. Uploading a dataframe (as a csv for now) inside a blob in one of the container of the storage account. Download a csv from a blob in one of the container of the storage account and render it as a pandas dataframe. Parameters: Name Type Description Default storage_acct_name str The name of the storage account to which you want to connect. required storage_acct_key str The account key of the storage account. required Source code in azure_helper/utils/blob_storage_interface.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def __init__ ( self , storage_acct_name : str , storage_acct_key : str ): \"\"\"Class responsible to interact with an existing Azure Storage Account. It uses a connection string to connect to the Storage Account. ```python conn_str = ( \"DefaultEndpointsProtocol=https;\" + f\"AccountName={storage_acct_name};\" + f\"AccountKey={storage_acct_key};\" + \"EndpointSuffix=core.windows.net\" ) ``` !!! info \"Information\" To get the key of this storage account we use the following command with the azure-cli. ```sh az storage account keys list --account-name <storage-account-name> --resource-group <resource-group> ``` This class is responsible for : * Creating a container in the storage account. * Uploading a dataframe (as a `csv` for now) inside a blob in one of the container of the storage account. * Download a `csv` from a blob in one of the container of the storage account and render it as a pandas dataframe. Args: storage_acct_name (str): The name of the storage account to which you want to connect. storage_acct_key (str): The account key of the storage account. \"\"\" conn_str = ( \"DefaultEndpointsProtocol=https;\" + f \"AccountName= { storage_acct_name } ;\" + f \"AccountKey= { storage_acct_key } ;\" + \"EndpointSuffix=core.windows.net\" ) self . blob_service_client = BlobServiceClient . from_connection_string ( conn_str , ) create_container ( container_name ) Create a container inside the storage account. Parameters: Name Type Description Default container_name str the name of the container you want to create. This name can only contains alphanumeric numbers and dashes '-'. required Source code in azure_helper/utils/blob_storage_interface.py 55 56 57 58 59 60 61 62 63 64 65 66 67 def create_container ( self , container_name : str ): \"\"\"Create a container inside the storage account. Args: container_name (str): the name of the container you want to create. This name can only contains alphanumeric numbers and dashes '-'. \"\"\" try : self . blob_service_client . create_container ( container_name ) log . info ( f \"Creating blob storage container { container_name } .\" ) except ResourceExistsError : log . warning ( f \"Blob storage container { container_name } already exists.\" ) pass upload_df_to_blob ( dataframe , container_name , blob_path ) Upload a pandas dataframe as a csv file inside a blob. Eg the following code. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 from azure_helper.utils.blob_storage_interface import BlobStorageInterface blob_storage_interface = BlobStorageInterface ( storage_acct_name = \"workspaceperso5448820782\" , storage_acct_key = \"XXXXX-XXXX-XXXXX-XXXX\" , ) blob_storage_interface . create_container ( container_name = \"project-mlops-mk-5448820782\" ) blob_storage_interface . upload_df_to_blob ( dataframe = x_train , container_name = \"project-mlops-mk-5448820782\" , blob_path = \"train/x_train.csv\" , ) blob_storage_interface . upload_df_to_blob ( dataframe = x_train , container_name = \"project-mlops-mk-5448820782\" , blob_path = \"train/y_train.csv\" , ) Upload the dataframes x_train and y_train as x_train.csv and y_train.csv in the following way. 1 2 3 4 5 6 7 Storage_Account : workspaceperso5448820782 \u2502 \u251c\u2500\u2500 Container : project-mlops-mk-5448820782 \u2502 \u251c\u2500\u2500 blob : train \u2502 \u2502 \u251c\u2500\u2500 x_train.csv \u2502 \u2502 \u2514\u2500\u2500 y_train.csv \u2502 \u2514\u2500\u2500 blob : test Attention As of now, there is no data versioning . Meaning that if the blob_path already exists, it will be overwritten with new datas. Parameters: Name Type Description Default dataframe pd . DataFrame The dataframe you want to upload. required container_name str The name of the container on which you want to upload the dataframe. required blob_path str The path to the csv required Source code in azure_helper/utils/blob_storage_interface.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def upload_df_to_blob ( self , dataframe : pd . DataFrame , container_name : str , blob_path : str , ): \"\"\"Upload a pandas dataframe as a `csv` file inside a blob. Eg the following code. ```python from azure_helper.utils.blob_storage_interface import BlobStorageInterface blob_storage_interface = BlobStorageInterface( storage_acct_name=\"workspaceperso5448820782\", storage_acct_key=\"XXXXX-XXXX-XXXXX-XXXX\", ) blob_storage_interface.create_container(container_name=\"project-mlops-mk-5448820782\") blob_storage_interface.upload_df_to_blob( dataframe=x_train, container_name=\"project-mlops-mk-5448820782\", blob_path=\"train/x_train.csv\", ) blob_storage_interface.upload_df_to_blob( dataframe=x_train, container_name=\"project-mlops-mk-5448820782\", blob_path=\"train/y_train.csv\", ) ``` Upload the dataframes `x_train` and `y_train` as `x_train.csv` and `y_train.csv` in the following way. ```bash Storage_Account : workspaceperso5448820782 \u2502 \u251c\u2500\u2500 Container : project-mlops-mk-5448820782 \u2502 \u251c\u2500\u2500 blob : train \u2502 \u2502 \u251c\u2500\u2500 x_train.csv \u2502 \u2502 \u2514\u2500\u2500 y_train.csv \u2502 \u2514\u2500\u2500 blob : test ``` !!! attention \"Attention\" As of now, there is no **data versioning**. Meaning that if the `blob_path` already exists, it will be overwritten with new datas. Args: dataframe (pd.DataFrame): The dataframe you want to upload. container_name (str): The name of the container on which you want to upload the dataframe. blob_path (str): The path to the csv \"\"\" self . create_container ( container_name ) blob_client = self . blob_service_client . get_blob_client ( container = container_name , blob = blob_path , ) try : blob_client . upload_blob ( dataframe . to_csv ( index = False , header = True ) . encode (), ) log . info ( f \"Dataset uploaded at blob path : { blob_path } .\" ) except ResourceExistsError : log . warning ( f \"Blob path { blob_path } already contains datas. Now deleting old datas tu upload the new ones.\" , ) blob_client . delete_blob () blob_client . upload_blob ( dataframe . to_csv ( index = False , header = True ) . encode (), ) log . info ( f \"New dataset uploaded at blob path : { blob_path } .\" ) download_blob_to_df ( container_name , blob_path ) Download a csv file a the given blob_path location and renders it as a pandas datatrame. 1 2 3 4 5 6 7 Storage_Account : workspaceperso5448820782 \u2502 \u251c\u2500\u2500 Container : project-mlops-mk-5448820782 \u2502 \u251c\u2500\u2500 blob : train \u2502 \u2502 \u251c\u2500\u2500 x_train.csv \u2502 \u2502 \u2514\u2500\u2500 y_train.csv \u2502 \u2514\u2500\u2500 blob : test 1 2 3 4 5 6 7 8 9 10 11 from azure_helper.utils.blob_storage_interface import BlobStorageInterface blob_storage_interface = BlobStorageInterface ( storage_acct_name = \"workspaceperso5448820782\" , storage_acct_key = \"XXXXX-XXXX-XXXXX-XXXX\" , ) df = blob_storage_interface . download_blob_to_df ( container_name = \"project-mlops-mk-5448820782\" , blob_path = \"train/x_train.csv\" , ) Parameters: Name Type Description Default container_name str The name of the container. required blob_path str The path to the csv file. required Returns: Type Description pd . DataFrame pd.DataFrame: the csv file as a dataframe. Source code in azure_helper/utils/blob_storage_interface.py 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 def download_blob_to_df ( self , container_name : str , blob_path : str ) -> pd . DataFrame : \"\"\"Download a `csv` file a the given `blob_path` location and renders it as a pandas datatrame. ```bash Storage_Account : workspaceperso5448820782 \u2502 \u251c\u2500\u2500 Container : project-mlops-mk-5448820782 \u2502 \u251c\u2500\u2500 blob : train \u2502 \u2502 \u251c\u2500\u2500 x_train.csv \u2502 \u2502 \u2514\u2500\u2500 y_train.csv \u2502 \u2514\u2500\u2500 blob : test ``` ```python from azure_helper.utils.blob_storage_interface import BlobStorageInterface blob_storage_interface = BlobStorageInterface( storage_acct_name=\"workspaceperso5448820782\", storage_acct_key=\"XXXXX-XXXX-XXXXX-XXXX\", ) df = blob_storage_interface.download_blob_to_df( container_name=\"project-mlops-mk-5448820782\", blob_path=\"train/x_train.csv\", ) ``` Args: container_name (str): The name of the container. blob_path (str): The path to the `csv` file. Returns: pd.DataFrame: the `csv` file as a dataframe. \"\"\" blob_client = self . blob_service_client . get_blob_client ( container = container_name , blob = blob_path , ) stream = blob_client . download_blob () buffer = StringIO ( stream . content_as_text ()) dataframe = pd . read_csv ( buffer ) log . info ( f \"Download from { container_name } ended successfully.\" ) return dataframe","title":"Blob Storage Interface"},{"location":"utils/blob_storage_interface/#the-blob-storage-interface-class","text":"","title":"The Blob Storage Interface class"},{"location":"utils/blob_storage_interface/#azure_helper.utils.blob_storage_interface","text":"","title":"blob_storage_interface"},{"location":"utils/blob_storage_interface/#azure_helper.utils.blob_storage_interface.BlobStorageInterface","text":"Source code in azure_helper/utils/blob_storage_interface.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 class BlobStorageInterface : def __init__ ( self , storage_acct_name : str , storage_acct_key : str ): \"\"\"Class responsible to interact with an existing Azure Storage Account. It uses a connection string to connect to the Storage Account. ```python conn_str = ( \"DefaultEndpointsProtocol=https;\" + f\"AccountName={storage_acct_name};\" + f\"AccountKey={storage_acct_key};\" + \"EndpointSuffix=core.windows.net\" ) ``` !!! info \"Information\" To get the key of this storage account we use the following command with the azure-cli. ```sh az storage account keys list --account-name <storage-account-name> --resource-group <resource-group> ``` This class is responsible for : * Creating a container in the storage account. * Uploading a dataframe (as a `csv` for now) inside a blob in one of the container of the storage account. * Download a `csv` from a blob in one of the container of the storage account and render it as a pandas dataframe. Args: storage_acct_name (str): The name of the storage account to which you want to connect. storage_acct_key (str): The account key of the storage account. \"\"\" conn_str = ( \"DefaultEndpointsProtocol=https;\" + f \"AccountName= { storage_acct_name } ;\" + f \"AccountKey= { storage_acct_key } ;\" + \"EndpointSuffix=core.windows.net\" ) self . blob_service_client = BlobServiceClient . from_connection_string ( conn_str , ) def create_container ( self , container_name : str ): \"\"\"Create a container inside the storage account. Args: container_name (str): the name of the container you want to create. This name can only contains alphanumeric numbers and dashes '-'. \"\"\" try : self . blob_service_client . create_container ( container_name ) log . info ( f \"Creating blob storage container { container_name } .\" ) except ResourceExistsError : log . warning ( f \"Blob storage container { container_name } already exists.\" ) pass def upload_df_to_blob ( self , dataframe : pd . DataFrame , container_name : str , blob_path : str , ): \"\"\"Upload a pandas dataframe as a `csv` file inside a blob. Eg the following code. ```python from azure_helper.utils.blob_storage_interface import BlobStorageInterface blob_storage_interface = BlobStorageInterface( storage_acct_name=\"workspaceperso5448820782\", storage_acct_key=\"XXXXX-XXXX-XXXXX-XXXX\", ) blob_storage_interface.create_container(container_name=\"project-mlops-mk-5448820782\") blob_storage_interface.upload_df_to_blob( dataframe=x_train, container_name=\"project-mlops-mk-5448820782\", blob_path=\"train/x_train.csv\", ) blob_storage_interface.upload_df_to_blob( dataframe=x_train, container_name=\"project-mlops-mk-5448820782\", blob_path=\"train/y_train.csv\", ) ``` Upload the dataframes `x_train` and `y_train` as `x_train.csv` and `y_train.csv` in the following way. ```bash Storage_Account : workspaceperso5448820782 \u2502 \u251c\u2500\u2500 Container : project-mlops-mk-5448820782 \u2502 \u251c\u2500\u2500 blob : train \u2502 \u2502 \u251c\u2500\u2500 x_train.csv \u2502 \u2502 \u2514\u2500\u2500 y_train.csv \u2502 \u2514\u2500\u2500 blob : test ``` !!! attention \"Attention\" As of now, there is no **data versioning**. Meaning that if the `blob_path` already exists, it will be overwritten with new datas. Args: dataframe (pd.DataFrame): The dataframe you want to upload. container_name (str): The name of the container on which you want to upload the dataframe. blob_path (str): The path to the csv \"\"\" self . create_container ( container_name ) blob_client = self . blob_service_client . get_blob_client ( container = container_name , blob = blob_path , ) try : blob_client . upload_blob ( dataframe . to_csv ( index = False , header = True ) . encode (), ) log . info ( f \"Dataset uploaded at blob path : { blob_path } .\" ) except ResourceExistsError : log . warning ( f \"Blob path { blob_path } already contains datas. Now deleting old datas tu upload the new ones.\" , ) blob_client . delete_blob () blob_client . upload_blob ( dataframe . to_csv ( index = False , header = True ) . encode (), ) log . info ( f \"New dataset uploaded at blob path : { blob_path } .\" ) def download_blob_to_df ( self , container_name : str , blob_path : str ) -> pd . DataFrame : \"\"\"Download a `csv` file a the given `blob_path` location and renders it as a pandas datatrame. ```bash Storage_Account : workspaceperso5448820782 \u2502 \u251c\u2500\u2500 Container : project-mlops-mk-5448820782 \u2502 \u251c\u2500\u2500 blob : train \u2502 \u2502 \u251c\u2500\u2500 x_train.csv \u2502 \u2502 \u2514\u2500\u2500 y_train.csv \u2502 \u2514\u2500\u2500 blob : test ``` ```python from azure_helper.utils.blob_storage_interface import BlobStorageInterface blob_storage_interface = BlobStorageInterface( storage_acct_name=\"workspaceperso5448820782\", storage_acct_key=\"XXXXX-XXXX-XXXXX-XXXX\", ) df = blob_storage_interface.download_blob_to_df( container_name=\"project-mlops-mk-5448820782\", blob_path=\"train/x_train.csv\", ) ``` Args: container_name (str): The name of the container. blob_path (str): The path to the `csv` file. Returns: pd.DataFrame: the `csv` file as a dataframe. \"\"\" blob_client = self . blob_service_client . get_blob_client ( container = container_name , blob = blob_path , ) stream = blob_client . download_blob () buffer = StringIO ( stream . content_as_text ()) dataframe = pd . read_csv ( buffer ) log . info ( f \"Download from { container_name } ended successfully.\" ) return dataframe","title":"BlobStorageInterface"},{"location":"utils/blob_storage_interface/#azure_helper.utils.blob_storage_interface.BlobStorageInterface.__init__","text":"Class responsible to interact with an existing Azure Storage Account. It uses a connection string to connect to the Storage Account. 1 2 3 4 5 6 conn_str = ( \"DefaultEndpointsProtocol=https;\" + f \"AccountName= { storage_acct_name } ;\" + f \"AccountKey= { storage_acct_key } ;\" + \"EndpointSuffix=core.windows.net\" ) Information To get the key of this storage account we use the following command with the azure-cli. 1 az storage account keys list --account-name <storage-account-name> --resource-group <resource-group> This class is responsible for : Creating a container in the storage account. Uploading a dataframe (as a csv for now) inside a blob in one of the container of the storage account. Download a csv from a blob in one of the container of the storage account and render it as a pandas dataframe. Parameters: Name Type Description Default storage_acct_name str The name of the storage account to which you want to connect. required storage_acct_key str The account key of the storage account. required Source code in azure_helper/utils/blob_storage_interface.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def __init__ ( self , storage_acct_name : str , storage_acct_key : str ): \"\"\"Class responsible to interact with an existing Azure Storage Account. It uses a connection string to connect to the Storage Account. ```python conn_str = ( \"DefaultEndpointsProtocol=https;\" + f\"AccountName={storage_acct_name};\" + f\"AccountKey={storage_acct_key};\" + \"EndpointSuffix=core.windows.net\" ) ``` !!! info \"Information\" To get the key of this storage account we use the following command with the azure-cli. ```sh az storage account keys list --account-name <storage-account-name> --resource-group <resource-group> ``` This class is responsible for : * Creating a container in the storage account. * Uploading a dataframe (as a `csv` for now) inside a blob in one of the container of the storage account. * Download a `csv` from a blob in one of the container of the storage account and render it as a pandas dataframe. Args: storage_acct_name (str): The name of the storage account to which you want to connect. storage_acct_key (str): The account key of the storage account. \"\"\" conn_str = ( \"DefaultEndpointsProtocol=https;\" + f \"AccountName= { storage_acct_name } ;\" + f \"AccountKey= { storage_acct_key } ;\" + \"EndpointSuffix=core.windows.net\" ) self . blob_service_client = BlobServiceClient . from_connection_string ( conn_str , )","title":"__init__()"},{"location":"utils/blob_storage_interface/#azure_helper.utils.blob_storage_interface.BlobStorageInterface.create_container","text":"Create a container inside the storage account. Parameters: Name Type Description Default container_name str the name of the container you want to create. This name can only contains alphanumeric numbers and dashes '-'. required Source code in azure_helper/utils/blob_storage_interface.py 55 56 57 58 59 60 61 62 63 64 65 66 67 def create_container ( self , container_name : str ): \"\"\"Create a container inside the storage account. Args: container_name (str): the name of the container you want to create. This name can only contains alphanumeric numbers and dashes '-'. \"\"\" try : self . blob_service_client . create_container ( container_name ) log . info ( f \"Creating blob storage container { container_name } .\" ) except ResourceExistsError : log . warning ( f \"Blob storage container { container_name } already exists.\" ) pass","title":"create_container()"},{"location":"utils/blob_storage_interface/#azure_helper.utils.blob_storage_interface.BlobStorageInterface.upload_df_to_blob","text":"Upload a pandas dataframe as a csv file inside a blob. Eg the following code. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 from azure_helper.utils.blob_storage_interface import BlobStorageInterface blob_storage_interface = BlobStorageInterface ( storage_acct_name = \"workspaceperso5448820782\" , storage_acct_key = \"XXXXX-XXXX-XXXXX-XXXX\" , ) blob_storage_interface . create_container ( container_name = \"project-mlops-mk-5448820782\" ) blob_storage_interface . upload_df_to_blob ( dataframe = x_train , container_name = \"project-mlops-mk-5448820782\" , blob_path = \"train/x_train.csv\" , ) blob_storage_interface . upload_df_to_blob ( dataframe = x_train , container_name = \"project-mlops-mk-5448820782\" , blob_path = \"train/y_train.csv\" , ) Upload the dataframes x_train and y_train as x_train.csv and y_train.csv in the following way. 1 2 3 4 5 6 7 Storage_Account : workspaceperso5448820782 \u2502 \u251c\u2500\u2500 Container : project-mlops-mk-5448820782 \u2502 \u251c\u2500\u2500 blob : train \u2502 \u2502 \u251c\u2500\u2500 x_train.csv \u2502 \u2502 \u2514\u2500\u2500 y_train.csv \u2502 \u2514\u2500\u2500 blob : test Attention As of now, there is no data versioning . Meaning that if the blob_path already exists, it will be overwritten with new datas. Parameters: Name Type Description Default dataframe pd . DataFrame The dataframe you want to upload. required container_name str The name of the container on which you want to upload the dataframe. required blob_path str The path to the csv required Source code in azure_helper/utils/blob_storage_interface.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def upload_df_to_blob ( self , dataframe : pd . DataFrame , container_name : str , blob_path : str , ): \"\"\"Upload a pandas dataframe as a `csv` file inside a blob. Eg the following code. ```python from azure_helper.utils.blob_storage_interface import BlobStorageInterface blob_storage_interface = BlobStorageInterface( storage_acct_name=\"workspaceperso5448820782\", storage_acct_key=\"XXXXX-XXXX-XXXXX-XXXX\", ) blob_storage_interface.create_container(container_name=\"project-mlops-mk-5448820782\") blob_storage_interface.upload_df_to_blob( dataframe=x_train, container_name=\"project-mlops-mk-5448820782\", blob_path=\"train/x_train.csv\", ) blob_storage_interface.upload_df_to_blob( dataframe=x_train, container_name=\"project-mlops-mk-5448820782\", blob_path=\"train/y_train.csv\", ) ``` Upload the dataframes `x_train` and `y_train` as `x_train.csv` and `y_train.csv` in the following way. ```bash Storage_Account : workspaceperso5448820782 \u2502 \u251c\u2500\u2500 Container : project-mlops-mk-5448820782 \u2502 \u251c\u2500\u2500 blob : train \u2502 \u2502 \u251c\u2500\u2500 x_train.csv \u2502 \u2502 \u2514\u2500\u2500 y_train.csv \u2502 \u2514\u2500\u2500 blob : test ``` !!! attention \"Attention\" As of now, there is no **data versioning**. Meaning that if the `blob_path` already exists, it will be overwritten with new datas. Args: dataframe (pd.DataFrame): The dataframe you want to upload. container_name (str): The name of the container on which you want to upload the dataframe. blob_path (str): The path to the csv \"\"\" self . create_container ( container_name ) blob_client = self . blob_service_client . get_blob_client ( container = container_name , blob = blob_path , ) try : blob_client . upload_blob ( dataframe . to_csv ( index = False , header = True ) . encode (), ) log . info ( f \"Dataset uploaded at blob path : { blob_path } .\" ) except ResourceExistsError : log . warning ( f \"Blob path { blob_path } already contains datas. Now deleting old datas tu upload the new ones.\" , ) blob_client . delete_blob () blob_client . upload_blob ( dataframe . to_csv ( index = False , header = True ) . encode (), ) log . info ( f \"New dataset uploaded at blob path : { blob_path } .\" )","title":"upload_df_to_blob()"},{"location":"utils/blob_storage_interface/#azure_helper.utils.blob_storage_interface.BlobStorageInterface.download_blob_to_df","text":"Download a csv file a the given blob_path location and renders it as a pandas datatrame. 1 2 3 4 5 6 7 Storage_Account : workspaceperso5448820782 \u2502 \u251c\u2500\u2500 Container : project-mlops-mk-5448820782 \u2502 \u251c\u2500\u2500 blob : train \u2502 \u2502 \u251c\u2500\u2500 x_train.csv \u2502 \u2502 \u2514\u2500\u2500 y_train.csv \u2502 \u2514\u2500\u2500 blob : test 1 2 3 4 5 6 7 8 9 10 11 from azure_helper.utils.blob_storage_interface import BlobStorageInterface blob_storage_interface = BlobStorageInterface ( storage_acct_name = \"workspaceperso5448820782\" , storage_acct_key = \"XXXXX-XXXX-XXXXX-XXXX\" , ) df = blob_storage_interface . download_blob_to_df ( container_name = \"project-mlops-mk-5448820782\" , blob_path = \"train/x_train.csv\" , ) Parameters: Name Type Description Default container_name str The name of the container. required blob_path str The path to the csv file. required Returns: Type Description pd . DataFrame pd.DataFrame: the csv file as a dataframe. Source code in azure_helper/utils/blob_storage_interface.py 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 def download_blob_to_df ( self , container_name : str , blob_path : str ) -> pd . DataFrame : \"\"\"Download a `csv` file a the given `blob_path` location and renders it as a pandas datatrame. ```bash Storage_Account : workspaceperso5448820782 \u2502 \u251c\u2500\u2500 Container : project-mlops-mk-5448820782 \u2502 \u251c\u2500\u2500 blob : train \u2502 \u2502 \u251c\u2500\u2500 x_train.csv \u2502 \u2502 \u2514\u2500\u2500 y_train.csv \u2502 \u2514\u2500\u2500 blob : test ``` ```python from azure_helper.utils.blob_storage_interface import BlobStorageInterface blob_storage_interface = BlobStorageInterface( storage_acct_name=\"workspaceperso5448820782\", storage_acct_key=\"XXXXX-XXXX-XXXXX-XXXX\", ) df = blob_storage_interface.download_blob_to_df( container_name=\"project-mlops-mk-5448820782\", blob_path=\"train/x_train.csv\", ) ``` Args: container_name (str): The name of the container. blob_path (str): The path to the `csv` file. Returns: pd.DataFrame: the `csv` file as a dataframe. \"\"\" blob_client = self . blob_service_client . get_blob_client ( container = container_name , blob = blob_path , ) stream = blob_client . download_blob () buffer = StringIO ( stream . content_as_text ()) dataframe = pd . read_csv ( buffer ) log . info ( f \"Download from { container_name } ended successfully.\" ) return dataframe","title":"download_blob_to_df()"}]}